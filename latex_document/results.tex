\chapter{Results and evaluation} \label{chap:results}
The primary metric for the results of this project is the validation accuracy of the models. Validation accuracy is calculated as
the amount of correct predictions divided by the total number of predictions in the validation dataset as can be seen in the equation below:

An additional metric specified is the loss of each model, which has been described in \autoref{sec:model_fitting} and was
utilised to detect overfitting in the various models tested.

The results have also been compared to a benchmark application using an existing case study to the 
best performing existing research method as found in the results. 

\section{Table of all results}
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Iteration & Research Method & Best Validation Accuracy \\
        \hline\hline
        \hyperref[iteration1_results]{Iteration 1} & LSTM & 55.54\%\\
        \hyperref[iteration2_results]{Iteration 2} & CNN & 53.57\%\\
        \hyperref[iteration3_results]{Iteration 3} & CNN-LSTM (sequential) & 56.07\%\\
        \hyperref[iteration4_results]{Iteration 4} & \makecell{LSTM + CNN +\\ CNN-LSTM (parallel)} & 53.21\%\\
        \hyperref[iteration5_results]{Iteration 5} & CNN-LSTM (sequential) & 57.50\%\\
        \hyperref[benchmark_results]{Benchmark} & CNN-LSTM (sequential) & 54.55\%\\
        \hline
    \end{tabular}
    \caption{Table showing the best model's validation accuracy of each iteration}
    \label{tab:all_results_overview}
\end{table}
\FloatBarrier

\section{Iteration 1 results}\label{iteration1_results}
As mentioned in \autoref{ssec:iteration1layers}, various combinations of LSTM layers and Dense layers were tested,
each with various sizes. The results of all the models can be seen in \autoref{fig:iteration1_train_accuracy}
and \autoref{fig:iteration1_all_accuracy}.\\
The results of the best model can be seen in \autoref{fig:iteration1_best_accuracy}.

\subsection{Accuracies of all tests}
In the charts below, each line represents a combination of different layer types and sizes of the Dense and LSTM layers of the chart.
For both the training and validation accuracy charts, each line has a corresponding line in the loss charts with the same colour.
There are 16 lines in total for this iteration as there were 2 LSTM layer amounts, 2 Dense layer amounts,
2 LSTM layer sizes, and 2 Dense layer sizes in the tests.

\subsubsection{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/lstm/lstm_all_acc_t.pdf}
    \caption[Training accuracies for Iteration 1]{Figure of all training accuracies of the combinations of LSTM Layers and Dense Layers in Iteration 1}
    \label{fig:iteration1_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/lstm/lstm_all_loss_t.pdf}
    \caption[Training losses for Iteration 1]{Figure of all training losses of the combinations of LSTM Layers and Dense Layers in Iteration 1}
    \label{fig:iteration1_train_loss}
\end{figure}
\FloatBarrier

\subsubsection{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/lstm/lstm_all_acc.pdf}
    \caption[Validation accuracies for Iteration 1]{Figure of all validation accuracies of the combinations of LSTM Layers and Dense Layers in Iteration 1}
    \label{fig:iteration1_all_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/lstm/lstm_all_loss.pdf}
    \caption[Validation losses for Iteration 1]{Figure of all validation losses of the combinations of LSTM Layers and Dense Layers in Iteration 1}
    \label{fig:iteration1_all_loss}
\end{figure}
\FloatBarrier

There are varying degrees of success with different combinations of layers; some do not improve in accuracy and others do.
The validation losses using the sparse categorical loss function as described earlier in \autoref{sec:model_fitting}
were used to help identify which models had the lowest error rate; and as an increasing validation
loss is an indicator of overfitting, it was used to filter models showing overfitting.

\subsection{Accuracies of the best result}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/lstm/lstm_2L64-2D32_acc.pdf}
    \caption[Best accuracy for Iteration 1]{Figure of the best accuracy of the combinations of LSTM Layers and Dense Layers in Iteration 1}
    \label{fig:iteration1_best_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/lstm/lstm_2L64-2D32_loss.pdf}
    \caption[Best loss for Iteration 1]{Figure of the best loss of the combinations of LSTM Layers and Dense Layers in Iteration 1}
    \label{fig:iteration1_best_loss}
\end{figure}
\FloatBarrier

The best model found within this iteration was one with a two LSTM layers of size 64 followed
by the final Dense layer of size 2. The grey line represents the training set and the orange line represents
the validation set. The validation loss shows little overfitting as can be seen in
\autoref{fig:iteration1_best_loss}. At 31 epochs, the validation accuracy reaches its highest value of
55.54\% whilst the training accuracy reached above 57.18\%.
\subsection{Evaluation of iteration 1}
With a validation accuracy above 55\% it suggests that there is improvement above a random choice. It forms a good
basis for testing further AI models, specifically the CNN-LSTM hybrid model.
There are various limitations regarding iteration 1. This iteration does not test various combinations of
input features nor does it test various sequence lengths. Furthermore, there could be another number of layers or
layer sizes that is better suited to the problem, but unfortunately many have not been tested due to time constraints.

\section{Iteration 2 results}\label{iteration2_results}
As mentioned in \autoref{ssec:iteration2layers}, various combinations of Conv1D layers and Dense layers were tested,
each with various sizes. The results of all the models can be seen in \autoref{fig:iteration2_train_accuracy}
and \autoref{fig:iteration2_all_accuracy}.\\
The results of the best model can be seen in \autoref{fig:iteration2_best_accuracy}.

\subsection{Accuracies of all tests}
In the charts below, each line represents a combination of different layer types and sizes of the Dense and LSTM layers of the chart.
For both the training and validation accuracy charts, each line has a corresponding line in the loss charts with the same colour.
There are 16 lines in total for this iteration as there were 2 Conv1D layer amounts, 2 Dense layer amounts,
2 Conv1D layer sizes, and 2 Dense layer sizes in the tests.

\subsubsection{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnn/cnn_all_acc_t.pdf}
    \caption[Training accuracies for Iteration 2]{Figure of all training accuracies of the combinations of Conv1D Layers and Dense Layers in Iteration 2}
    \label{fig:iteration2_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnn/cnn_all_loss_t.pdf}
    \caption[Training losses for Iteration 2]{Figure of all training losses of the combinations of Conv1D Layers and Dense Layers in Iteration 2}
    \label{fig:iteration2_train_loss}
\end{figure}
\FloatBarrier

\subsubsection{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnn/cnn_all_acc.pdf}
    \caption[Validation accuracies for Iteration 2]{Figure of all validation accuracies of the combinations of Conv1D Layers and Dense Layers in Iteration 2}
    \label{fig:iteration2_all_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnn/cnn_all_loss.pdf}
    \caption[Validation losses for Iteration 2]{Figure of all validation losses of the combinations of Conv1D Layers and Dense Layers in Iteration 2}
    \label{fig:iteration2_all_loss}
\end{figure}
\FloatBarrier

There are varying degrees of success with different combinations of layers; some do not improve in accuracy and others do.
The validation losses using the sparse categorical loss function as described earlier in \autoref{sec:model_fitting}
were used to help identify which models had the lowest error rate; and as an increasing validation
loss is an indicator of overfitting, it was used to filter models showing overfitting.

\subsection{Accuracies of the best result}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnn/cnn_1C32-1D64_acc.pdf}
    \caption[Best accuracy for Iteration 2]{Figure of the best accuracy of the combinations of Conv1D Layers and Dense Layers in Iteration 2}
    \label{fig:iteration2_best_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnn/cnn_1C32-1D64_loss.pdf}
    \caption[Best loss for Iteration 2]{Figure of the best loss of the combinations of Conv1D Layers and Dense Layers in Iteration 2}
    \label{fig:iteration2_best_loss}
\end{figure}
\FloatBarrier

The best model found within this iteration was one with a one CNN layer of size 32 followed
by the final Dense layer of size 2. The blue line represents the training set and the pink line represents
the validation set. The validation loss shows little overfitting as can be seen in
\autoref{fig:iteration2_best_loss}. At 5 epochs, the validation accuracy reaches its highest value of
53.57\% whilst the training accuracy reached above 55.31\%.
\subsection{Evaluation of iteration 2}
With a validation accuracy above 53\% it suggests that there is only minimal improvement above a random choice. It forms a good
basis for testing further AI models, specifically the CNN-LSTM hybrid model.
There are various limitations regarding iteration 2. This iteration does not test various combinations of
input features nor does it test various sequence lengths. Furthermore, there could be another number of layers or
layer sizes that is better suited to the problem, but unfortunately many have not been tested due to time constraints.

\section{Iteration 3 results}\label{iteration3_results}
As mentioned in \autoref{ssec:iteration3layers}, various combinations of Conv1D layers, LSTM layers and Dense layers were tested,
each with various sizes. The results of all the models can be seen in \autoref{fig:iteration3_train_accuracy}
and \autoref{fig:iteration3_all_accuracy}.\\
The results of the best model can be seen in \autoref{fig:iteration3_best_accuracy}.

\subsection{Accuracies of all tests}
In the charts below, each line represents a combination of different layer types and sizes of the Conv1D, LSTM and Dense layers.
For both the training and validation accuracy charts, each line has a corresponding line in the loss charts with the same colour.
There are 64 lines in total for this iteration as there were 2 Conv1D layer amounts, 2 LSTM layer amounts, 2 Dense layer amounts,
as well as 2 Conv1D layer sizes, 2 LSTM layer sizes and 2 Dense layer sizes in the tests.

\subsubsection{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnnlstm/cnnlstm_all_acc_t.pdf}
    \caption[Training accuracies for Iteration 3]{Figure of all training accuracies of the combinations of Conv1D, LSTM and Dense Layers in Iteration 3}
    \label{fig:iteration3_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnnlstm/cnnlstm_all_loss_t.pdf}
    \caption[Training losses for Iteration 3]{Figure of all training losses of the combinations of Conv1D, LSTM and Dense Layers in Iteration 3}
    \label{fig:iteration3_train_loss}
\end{figure}
\FloatBarrier

\subsubsection{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnnlstm/cnnlstm_all_acc.pdf}
    \caption[Validation accuracies for Iteration 3]{Figure of all validation accuracies of the combinations of Conv1D, LSTM and Dense Layers in Iteration 3}
    \label{fig:iteration3_all_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnnlstm/cnnlstm_all_loss.pdf}
    \caption[Validation losses for Iteration 3]{Figure of all validation losses of the combinations of Conv1D, LSTM and Dense Layers in Iteration 3}
    \label{fig:iteration3_all_loss}
\end{figure}
\FloatBarrier

There are varying degrees of success with different combinations of layers; some do not improve in accuracy and others do.
The validation losses using the sparse categorical loss function as described earlier in \autoref{sec:model_fitting}
were used to help identify which models had the lowest error rate; and as an increasing validation
loss is an indicator of overfitting, it was used to filter models showing overfitting.

\subsection{Accuracies of the best result}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnnlstm/cnnlstm_2C32-1L16-2D64_acc.pdf}
    \caption[Best accuracy for Iteration 3]{Figure of the best accuracy of the combinations of Conv1D, LSTM and Dense Layers in Iteration 3}
    \label{fig:iteration3_best_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/cnnlstm/cnnlstm_2C32-1L16-2D64_loss.pdf}
    \caption[Best loss for Iteration 3]{Figure of the best loss of the combinations of Conv1D, LSTM and Dense Layers in Iteration 3}
    \label{fig:iteration3_best_loss}
\end{figure}
\FloatBarrier

The best model found within this iteration was one with a two Conv1D layers of size 32 followed by 1 LSTM layer
of size 16, and a Dense layer of size 64 followed by a final Dense layer of size 2.
The blue line represents the training set and the pink line represents
the validation set. The validation loss shows little overfitting as can be seen in
\autoref{fig:iteration3_best_loss}. At 19 epochs, the validation accuracy reaches its highest value of
56.07\% whilst the training accuracy reached 55.24\%. The validation accuracy being somewhat greater than the
training accuracy can be explained by the dropout layers within the model that can affect the results of the training
set but does not affect the validation set.

\subsection{Evaluation of iteration 3}
With a validation accuracy above 56.07\%, it is currently the best model when compared to the previous iterations.
This suggests there is a significant advantage compared to randomly guessing the next trading day's direction.
Furthermore, this aligns with the studies analysed in the literature review. However, this iteration alone cannot be
used to answer the research questions as it does not test different sequence lengths or combinations of input features.
Additionally, there could be a different number of layers or sizes of layers that perform better but unfortunately could
not be tested due to time constraints.

\section{Iteration 4 results}\label{iteration4_results}
As mentioned in \autoref{ssec:iteration4_ai_model}, a concatenated model of the best models of the previous 
three iterations were combined in parallel. The results of this model can be seen in
\autoref{fig:iteration4_accuracy}.

\subsection{Accuracies of iteration 4 model}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/concat/concat_acc.pdf}
    \caption[Accuracies for Iteration 4]{Figure of all training and validation accuracies in Iteration 4}
    \label{fig:iteration4_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/concat/concat_loss.pdf}
    \caption[Losses for Iteration 4]{Figure of all training and validation losses in Iteration 4}
    \label{fig:iteration4_loss}
\end{figure}
\FloatBarrier

The orange line represents the training set and the blue line represents
the validation set. The validation loss shows little overfitting as can be seen in
\autoref{fig:iteration4_loss} as the loss does not significantly increase. At 3 epochs,
the validation accuracy reaches its highest value of 53.21\% whilst the training accuracy reached 52.82\%. The validation accuracy being somewhat greater than the
training accuracy can be explained by the dropout layers within the model that can affect the results of the training
set but does not affect the validation set.

\subsection{Evaluation of iteration 4}
With a validation accuracy above 53\%, there is only a minimal improvement above a random guess of the direction
of the next trading day. This model was included to test between a sequential hybrid model as seen in Iteration 3
and this parallel hybrid approach. However, this model does not offer any advantage over the CNN-LSTM sequential
model alone; due to this, no further tests will be utilised with this model.

\section{Iteration 5 results}\label{iteration5_results}
This iteration primarily used a similar model to that in Iteration 3, however with varied input features
and varied sequence lengths. There are various results that come from this model from identifying the
best amount of historic data to identifying the most important inputs.

\subsection{Accuracies of all tests}

For each of the tests in Iteration 5, the charts contain 15 lines representing the different combinations
of input features as mentioned in \autoref{ssec:iteration5_input_features}. Each line in the accuracy chart
has a corresponding line in the loss chart with the same colour. Each line in the training charts has a 
corresponding line in the validation charts with a different colour.\\
\\
The best values of the validation accuracies of each model at any epoch are shown in
\autoref{ssec:iteration5_best_val_acc}.

\subsubsection{Two days of historic data accuracies and losses}
\textbf{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_days_acc_t.pdf}
    \caption[Training accuracies for Iteration 5 with two days of historic data]{Figure of all training accuracies with two days of historic data in Iteration 5}
    \label{fig:iteration5_two_days_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_days_loss_t.pdf}
    \caption[Training losses for Iteration 5 with two days of historic data]{Figure of all training losses with two days of historic data in Iteration 5}
    \label{fig:iteration5_two_days_train_loss}
\end{figure}
\FloatBarrier

\pagebreak
\textbf{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_days_acc.pdf}
    \caption[Validation accuracies for Iteration 5 with two days of historic data]{Figure of all validation accuracies with two days of historic data in Iteration 5}
    \label{fig:iteration5_two_days_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_days_loss.pdf}
    \caption[Validation losses for Iteration 5 with two days of historic data]{Figure of all validation losses with two days of historic data in Iteration 5}
    \label{fig:iteration5_two_days_loss}
\end{figure}
\FloatBarrier

The greatest validation accuracy for the model with a sequence length of two days was 55.54\% with the dataset using price and volatility data.
While the chart above in \autoref{fig:iteration5_two_days_loss} may look erratic, the changes to the loss are minimal due to the scale as it only varies
between 0.689 and 0.697. This suggests there is little overfitting as it does not increase significantly.

\pagebreak
\subsubsection{One week of historic data accuracies and losses}
\textbf{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/week_acc_t.pdf}
    \caption[Training accuracies for Iteration 5 with one week of historic data]{Figure of all training accuracies with one week of historic data in Iteration 5}
    \label{fig:iteration5_week_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/week_loss_t.pdf}
    \caption[Training losses for Iteration 5 with one week of historic data]{Figure of all training losses with one week of historic data in Iteration 5}
    \label{fig:iteration5_week_train_loss}
\end{figure}
\FloatBarrier

\pagebreak
\textbf{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/week_acc.pdf}
    \caption[Validation accuracies for Iteration 5 with one week of historic data]{Figure of all validation accuracies with one week of historic data in Iteration 5}
    \label{fig:iteration5_week_accuracy}
\end{figure}

\FloatBarrier
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/week_loss.pdf}
    \caption[Validation losses for Iteration 5 with one week of historic data]{Figure of all validation losses with one week of historic data in Iteration 5}
    \label{fig:iteration5_week_loss}
\end{figure}
\FloatBarrier

The greatest validation accuracy for the model with a sequence length of one week was 55.54\% with the dataset using price and repurchase
agreements (repo) data.
While the chart above in \autoref{fig:iteration5_week_loss} may look erratic, the changes to the loss are minimal due to the scale as it only varies
between 0.688 and 0.713. This suggests there is little overfitting as it does not increase significantly.

\subsubsection{Two weeks of historic data accuracies and losses}
\textbf{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_weeks_acc_t.pdf}
    \caption[Training accuracies for Iteration 5 with two weeks of historic data]{Figure of all training accuracies with two weeks of historic data in Iteration 5}
    \label{fig:iteration5_two_weeks_train_accuracy}
\end{figure}

\FloatBarrier
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_weeks_loss_t.pdf}
    \caption[Training losses for Iteration 5 with two weeks of historic data]{Figure of all training losses with two weeks of historic data in Iteration 5}
    \label{fig:iteration5_two_weeks_train_loss}
\end{figure}
\FloatBarrier

\pagebreak
\textbf{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_weeks_acc.pdf}
    \caption[Validation accuracies for Iteration 5 with two weeks of historic data]{Figure of all validation accuracies with two weeks of historic data in Iteration 5}
    \label{fig:iteration5_two_weeks_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_weeks_loss.pdf}
    \caption[Validation losses for Iteration 5 with two weeks of historic data]{Figure of all validation losses with two weeks of historic data in Iteration 5}
    \label{fig:iteration5_two_weeks_loss}
\end{figure}
\FloatBarrier

The greatest validation accuracy for the model with a sequence length of two weeks was 55.18\% with the dataset using price and repurchase
agreements (repo) data.
While the chart above in \autoref{fig:iteration5_two_weeks_loss} may look erratic, the changes to the loss are minimal due to the scale as it only varies
between 0.688 and 0.697. This suggests there is little overfitting as it does not increase significantly.

\subsubsection{One month of historic data accuracies and losses}
\textbf{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/month_acc_t.pdf}
    \caption[Training accuracies for Iteration 5 with one month of historic data]{Figure of all training accuracies with one month of historic data in Iteration 5}
    \label{fig:iteration5_month_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/month_loss_t.pdf}
    \caption[Training losses for Iteration 5 with one month of historic data]{Figure of all training losses with one month of historic data in Iteration 5}
    \label{fig:iteration5_month_train_loss}
\end{figure}
\FloatBarrier

\pagebreak
\textbf{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/month_acc.pdf}
    \caption[Validation accuracies for Iteration 5 with one month of historic data]{Figure of all validation accuracies with one month of historic data in Iteration 5}
    \label{fig:iteration5_month_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/month_loss.pdf}
    \caption[Validation losses for Iteration 5 with one month of historic data]{Figure of all validation losses with one month of historic data in Iteration 5}
    \label{fig:iteration5_month_loss}
\end{figure}
\FloatBarrier

The greatest validation accuracy for the model with a sequence length of two weeks was 57.50\% with the dataset using price
and treasury yields data.
While the chart above in \autoref{fig:iteration5_month_loss} may look erratic, the changes to the loss are minimal due to the scale as it only varies
between 0.687 and 0.696. This suggests there is little overfitting as it does not increase significantly.

\pagebreak
\subsubsection{Two months of historic data accuracies and losses}
\textbf{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_months_acc_t.pdf}
    \caption[Training accuracies for Iteration 5 with two months of historic data]{Figure of all training accuracies with two months of historic data in Iteration 5}
    \label{fig:iteration5_two_months_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_months_loss_t.pdf}
    \caption[Training losses for Iteration 5 with two months of historic data]{Figure of all training losses with two months of historic data in Iteration 5}
    \label{fig:iteration5_two_months_train_loss}
\end{figure}
\FloatBarrier

\pagebreak
\textbf{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_months_acc.pdf}
    \caption[Validation accuracies for Iteration 5 with two months of historic data]{Figure of all validation accuracies with two months of historic data in Iteration 5}
    \label{fig:iteration5_two_months_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/two_months_loss.pdf}
    \caption[Validation losses for Iteration 5 with two months of historic data]{Figure of all validation losses with two months of historic data in Iteration 5}
    \label{fig:iteration5_two_months_loss}
\end{figure}
\FloatBarrier

The greatest validation accuracy for the model with a sequence length of two weeks was 54.11\% with the dataset using the extended price
changes dataset, the price and volatility data, or the price and treasury yields data.
While the chart above in \autoref{fig:iteration5_two_months_loss} may look erratic, the changes to the loss are minimal due to the scale as it only varies
between 0.689 and 0.703. This suggests there is little overfitting as it does not increase significantly.

\subsubsection{One quarter of historic data accuracies and losses}
\textbf{Training accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/quarter_acc_t.pdf}
    \caption[Training accuracies for Iteration 5 with one quarter of historic data]{Figure of all training accuracies with one quarter of historic data in Iteration 5}
    \label{fig:iteration5_quarter_train_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/quarter_loss_t.pdf}
    \caption[Training losses for Iteration 5 with one quarter of historic data]{Figure of all training losses with one quarter of historic data in Iteration 5}
    \label{fig:iteration5_quarter_train_loss}
\end{figure}
\FloatBarrier

\pagebreak
\textbf{Validation accuracies and losses}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/quarter_acc.pdf}
    \caption[Validation accuracies for Iteration 5 with one quarter of historic data]{Figure of all validation accuracies with one quarter of historic data in Iteration 5}
    \label{fig:iteration5_quarter_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.575\columnwidth]{figures/results/final/quarter_loss.pdf}
    \caption[Validation losses for Iteration 5 with one quarter of historic data]{Figure of all validation losses with one quarter of historic data in Iteration 5}
    \label{fig:iteration5_quarter_loss}
\end{figure}
\FloatBarrier

The greatest validation accuracy for the model with a sequence length of two weeks was 55.18\% with the dataset of the price and treasury yields data.
While the chart above in \autoref{fig:iteration5_quarter_loss} may look erratic, the changes to the loss are minimal due to the scale as it only varies
between 0.687 and 0.699. This suggests there is little overfitting as it does not increase significantly.

\subsection{Comparisons of models in iteration 5}\label{ssec:iteration5_best_val_acc}
The table below in \autoref{fig:final_val_accuracy} shows the accuracies of each combination of input features and length of historic data.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/final/final_val_accuracy.pdf}
    \caption[Best validation accuracies for Iteration 5]{Figure of all validation accuracies of each combination of input features and sequence length in Iteration 5}
    \label{fig:final_val_accuracy}
\end{figure}
\FloatBarrier

The results show the model of the stock market index price combined with treasury yield data and 
a sequence length of 1 month (21 days) of historic data, has the best accuracy with a value of
57.50\%.

However, there is additional data that can be inferred from the results. On average, the model with a sequence
length of two days performs the best with a validation accuracy of 53.84\%. This suggests that the stock markets may react
strongest to short term changes. The models' accuracies slightly decrease for the one-week sequence length, and fluctuate
slightly across other sequence lengths but never increase over the model with a sequence length of two days.

On average, the model utilising price and treasury yields data, have the highest average validation accuracy of 54.76\%. This
shows that treasury yields data can be an important factor to stock market returns.

In order to better visualise how different features affect the model, each combination of input features was compared against the `price only' dataset for each sequence length.
The results of these comparisions can be shown in the table below in \autoref{fig:val_accuracy_comparison}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/final/val_accuracy_comparison.pdf}
    \caption[Best validation accuracies for Iteration 5]{Figure of all validation accuracies of each combination of input features and sequence length in Iteration 5}
    \label{fig:val_accuracy_comparison}
\end{figure}
\FloatBarrier

The model with a sequence length of two days does not see many significant improvements with most combinations of input features.
For sequence lengths, many factors can have a positive impact to the models, the results above show that volume, volatility,
M1 money supply, GDP, Effective Federal Funds Rate (EFFR), gold prices, currency exchange rates, put-to-call ratio (options), and other
sentiment data. Whereas for sequence lengths of one month, there is some overlap of important factors that affect the accuracy of the models.
The factors that impact one month sequence lengths are the extended price dataset, as well as volume, volatility, M1 money supply, GDP, 
treasury yields, EFFR, Repurchase Agreements (Repo) utilisation and rates, Reverse Repo utilisation and rates, gold prices, currency exchange rates,
and put-to-call ratio (options) data.

\subsection{Evaluation of iteration 5}
The results from iteration 5 provided valuable data in order to be able to answer the research questions. With the best model having an accuracy
of 57.50\%, the results suggest that stock market movements are not always truly random and there are factors that affect stock prices. This
final iteration has provided great insight into which input features affect accuracy of stock market predictions the most, as well as what
sequence lengths of historic data as input. This iteration provides a model with greater accuracies than any model prior to it. The models in this
iteration also show very little overfitting as seen in only very minute fluctuations to the validation losses that do not increase significantly.

Due to time constraints and computational complexity, it was not possible to test various different models of different layers and layer sizes
for each combination of input features which may impact the results.

\section{Benchmark application results}\label{benchmark_results}
The benchmark application uses an existing case study of input features and applies it to the existing research method of the CNN-LSTM sequential
model as described in Iteration 3.

\subsection{Accuracies of all tests}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/benchmark/benchmark_acc.pdf}
    \caption[Accuracies for benchmark model]{Figure of training and validation accuracies in benchmark model}
    \label{fig:benchmark_accuracy}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/results/benchmark/benchmark_loss.pdf}
    \caption[Losses for benchmark model]{Figure of training and validation losses in benchmark model}
    \label{fig:benchmark_loss}
\end{figure}
\FloatBarrier

The orange line represents the training set and the blue line represents
the validation set. The validation loss shows little overfitting as can be seen in
\autoref{fig:iteration4_loss} as the loss does not significantly increase. At 10 epochs,
the validation accuracy reaches its highest value of 54.55\% whilst the training accuracy reached 50.18\%. The validation accuracy being somewhat greater than the
training accuracy can be explained by the dropout layers within the model that can affect the results of the training
set but does not affect the validation set. 

\subsection{Accuracies of the best result}


\subsection{Evaluation of benchmark model}
With a validation accuracy of 54.55\%, there is an improvement above a random guess of the direction
of the next trading day. This model was included to compare the results of the models of the artefact to understand
how well it performs to existing models.  This result suggests that the models of all five iterations of the artefact
outperform the existing case study as described in \autoref{ssec:benchmark_input_features}.


\section{Artefact evaluation}
The artefact in this project is shown to be novel based on the unique sets of input features as
it is the first time application of a novel case study to an existing research method. Generally,
the results from the models shown have been able to support in answering the research questions.

\subsection{Research questions evaluation}
\subsubsection{Predicting a stock market index's direction}
\textbf{Question} Is it possible to accurately predict a stock market index's (S\&P 500) direction for
the following day with an artificial intelligence model?\\
\textbf{Sub Questions}
\begin{itemize}
    \item If so, what is the accuracy of the model?
    \item If not, what limitations affected the model to cause it to be inaccurate?
    \item Can the findings of this question be used to prove/disprove the efficient market hypothesis?
\end{itemize}

The results from the artefact suggest that it is possible to predict the stock market index's direction
with a reasonable degree of accuracy. The best model of Iteration 5 had a validation accuracy of 57.5\%
which suggests there is often an ability to predict the daily direction of a stock market index. This
may also suggest there are often inefficiencies in the market and may offer some evidence to disprove
the efficient market hypothesis. While this model has been quite accurate, there are some limitations
involved that may affect the accuracy. The model uses data from November 2006 to March 2022; this involves
a relatively large time horizon and as a result, some ideas learned by the model may be outdated as the markets
evolve over time; for example settlement periods adjusting from T+3 days to T+2 days.

\subsubsection{Optimal amount of historical data as input}
\textbf{Question} How do different lengths of time as history for the input in the training data affect the model?\\
\textbf{Sub Questions}
\begin{itemize}
    \item What is the ideal length of time to be used in the training data?
    \item What are potential reasons for this length of time being the most useful?
\end{itemize}

The results from the artefact generally show that one month of historical data can provide the best models as seen in \autoref{fig:val_accuracy_comparison};
though for the input features tested, two days has the best results when averaged as the results show in \autoref{fig:final_val_accuracy}.
Generally, a short sequence length of historical data provides
the best result for predicting the daily direction of the stock market index with few input features affecting the accuracy of the
model. However, given an optimal set of input features, the model may be able to recognise patterns in longer sequence lengths.

The two day sequence length may be more accurate due to the fact markets are always adjusting and reacting to changes; and as a result
there may be short term inefficiencies that can be exploited. However, the one month sequence length accuracies may be explained due to
the fact many economical factors are reported by institutions / governments on a monthly basis. Furthermore, the one month sequence length
may be the best length for the LSTMs in the neural network model to identify and remember patterns that are beneficial to the model and
disregard any patterns that provide less value.

\subsubsection{Most important input features}
\textbf{Question} What are the most important input features that affect the model?\\
\textbf{Sub Questions}
\begin{itemize}
    \item How much do each of these input features contribute to the model?
    \item Do any of the input features identified have negligible impact to the model?
    \item What are the potential reasons these features do / do not impact the model?
\end{itemize}

On average, including treasury yields data to the model improved accuracies by 3.17\% compared to price alone as seen in
\autoref{fig:val_accuracy_comparison}.
The artefact suggests that information regarding the treasury yields have the most impact to the neural network models across all sequence
lengths apart from two days. 
This is likely due to the fact there generally aren't significant changes to treasury yields in a two day period; but they can
grow to be more significant over a long time horizon. The increased accuracy is likely due to the changes in yields may affect institutional decisions
depending on the risk management. Firms may reallocate funds between the stock market and treasury products; treasury products are generally less risky but
offer lesser returns than the stock market. Repurchase agreements (repos) show trends similar to the treasury yields. There generally aren't significant
changes in 2 days, but there may be patterns over longer periods of times. This may be due to the fact changes to repo markets
can affect liquidity in other markets.

There are several factors that do not impact the model greatly, such as reverse repurchase agreements (reverse repos).
This is likely due to the fact reverse repos were generally underutilised, especially in the training period and as a result did 
not affect the model significantly. Exchange volume, gold, currency exchange rates, inflation rates, and employment rates
generally did not affect the accuracy significantly. This is likely due to those factors remaining relatively stable most of the time. 

\subsection{Requirements evaluation}
\subsubsection{Functional requirements}
\textbf{Necessary functional requirements}
\begin{table}[ht]
    \centering
    \begin{tabular}{|p{100mm}|c|}
        \hline
        Requirement & Met? \\
        \hline\hline
        Predict daily price direction with 60\%+ accuracy & No \\
        Identify which input features are most important & Yes \\
        Identify what sequence length (number of days of history of each input feature) is optimal & Yes \\
        Present charts of accuracies and losses of each model & Yes \\
        \hline
    \end{tabular}
    \caption{Table showing whether necessary functional requirements were met}
    \label{tab:necessary_functional_requirements}
\end{table}
\FloatBarrier

Unfortunately, the models were not able to provide an accuracy greater than 60\%, but this did not
affect the models' ability to identify optimal features in stock market forecasting. Charts were
also able to produced using the Tensorboard callback.

\textbf{Optional functional requirements}
\begin{table}[ht]
    \centering
    \begin{tabular}{|p{100mm}|c|}
        \hline
        Requirement & Met? \\
        \hline\hline
        Predict daily price return with a mean absolute deviation of 2.5\% or lower & No \\
        \hline
    \end{tabular}
    \caption{Table showing whether optional functional requirements were met}
    \label{tab:optional_functional_requirements}
\end{table}
\FloatBarrier

This requirement was not met due to time constraints and decisions to focus on other models
predicting direction only. This decision was made due to the belief that understanding
the direction is more important than specific prices to generate a positive return on investment.

\subsubsection{Non-functional requirements}
\textbf{Necessary non-functional requirements}
\begin{table}[ht]
    \centering
    \begin{tabular}{|p{100mm}|c|}
        \hline
        Requirement & Met? \\
        \hline\hline
        The artefact should take less than one minute to run per model chosen (on modern Nvidia GPUs) & Yes* \\
        \hline
    \end{tabular}
    \caption{Table showing whether necessary non-functional requirements were met}
    \label{tab:necessary_non_functional_requirements}
\end{table}

*All models apart from concatenated parallel hybrid models were able to train the models in under a minute.
The models in Iteration 4 were not chosen to be used in the final iteration due to poor accuracies.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/time_taken.png}
    \caption[Time taken to train models]{Figure comparing the time taken to train models in Iteration 5 compared to Iteration 4}
    \label{fig:time_take_for_models}
\end{figure}
\FloatBarrier

\textbf{Optional non-functional requirements}
\begin{table}[ht]
    \centering
    \begin{tabular}{|p{100mm}|c|}
        \hline
        Requirement & Met? \\
        \hline\hline
        Front-end application that allows users to input features and sequence lengths to train different models & No \\
        User-facing documentation for artefact & No \\
        \hline
    \end{tabular}
    \caption{Table showing whether optional non-functional requirements were met}
    \label{tab:optional_non_functional_requirements}
\end{table}

Due to time constraints these requirements were not completed. Due to this being primarily a research project
rather than an engineering project, it was deemed unnecessary to have a front-end application. It is relatively
easy to adjust different models with the python code provided. The code also contains comments throughout and uses
relevant variable names to ensure readability and understanding. Furthermore, Tensorboard provides a good front-end
for visualising performance metrics of the models.