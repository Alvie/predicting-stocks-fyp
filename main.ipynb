{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform inputs to pandas dataframe\n",
    "\n",
    "## Stock Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPY Price & Volume\n",
    "#### https://uk.finance.yahoo.com/quote/SPY/history\n",
    "spy_df = pd.read_csv('./inputFeatures/stockIndex/SPY.csv',\n",
    "    index_col=[\"Date\"], \n",
    "    usecols=[\"Date\", \"SPYClose\", \"Volume\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### VIX (Volatility Index)\n",
    "#### https://uk.finance.yahoo.com/quote/%5EVIX/history\n",
    "vix_df = pd.read_csv('./inputFeatures/stockIndex/VIX.csv',\n",
    "    index_col=[\"Date\"], \n",
    "    usecols=[\"Date\", \"VIXClose\"],\n",
    "    parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### M1 Money Supply\n",
    "#### Board of Governors of the Federal Reserve System (US), M1 [WM1NS],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/WM1NS, March 31, 2022.\n",
    "m1_df = pd.read_csv('./inputFeatures/moneyAvailability/WM1NS.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### Employment Rate\n",
    "#### Organization for Economic Co-operation and Development,\n",
    "#### Employment Rate: Aged 15-64: All Persons for the United States\n",
    "#### [LREM64TTUSM156S], retrieved from FRED,\n",
    "#### Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/LREM64TTUSM156S, March 31, 2022.\n",
    "employment_df = pd.read_csv('./inputFeatures/moneyAvailability/EmploymentRate.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### Inflation Rate\n",
    "## data.bls.gov\n",
    "inflation_df = pd.read_csv('./inputFeatures/moneyAvailability/InflationRate.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### GDP Rate\n",
    "#### U.S. Bureau of Economic Analysis, Gross Domestic Product [GDP], retrieved from FRED,\n",
    "#### Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/GDP, March 31, 2022.\n",
    "gdp_df = pd.read_csv('./inputFeatures/moneyAvailability/GDP.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put Call Ratio\n",
    "#### https://www.alphalerts.com/live-historical-equity-pcr/\n",
    "pcr_df = pd.read_csv('./inputFeatures/sentimentIndicators/PCR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Consumer Sentiment\n",
    "#### Surveys of Consumers, University of Michigan: Consumer Sentiment Â© [UMCSENT]\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/UMCSENT, March 31, 2022.\n",
    "umcsent_df = pd.read_csv('./inputFeatures/sentimentIndicators/UMCSENT.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Consumer Confidence\n",
    "#### Organization for Economic Co-operation and Development, Organization for Economic Co-operation and Development:\n",
    "#### Main Economic Indicators (database),http://dx.doi.org/10.1787/data-00052-en\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/CSCICP03USM665S, March 31, 2022.\n",
    "confidence_df = pd.read_csv('./inputFeatures/sentimentIndicators/CSCICP03USM665S.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Treasury Yield Rates\n",
    "#### https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve\n",
    "treasury_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/daily-treasury-rates.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Effective Funds Rate\n",
    "#### Federal Reserve Bank of New York, Effective Federal Funds Rate [EFFR],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/EFFR, March 31, 2022.\n",
    "effr_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/EFFR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Accepted Repurchase Agreements (Repo) by the Federal Reserve\n",
    "#### Federal Reserve Bank of New York, Overnight Repurchase Agreements:\n",
    "#### Treasury Securities Purchased by the Federal Reserve in the Temporary Open Market Operations [RPONTSYD],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/RPONTSYD, March 31, 2022.\n",
    "#### https://www.newyorkfed.org/markets/desk-operations/repo\n",
    "repo_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/REPO.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Accepted Reverse Repurchase Agreements (Reverse Repo) by the Federal Reserve\n",
    "#### Federal Reserve Bank of New York, Overnight Reverse Repurchase Agreements:\n",
    "#### Treasury Securities Sold by the Federal Reserve in the Temporary Open Market Operations [RRPONTSYD],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/RRPONTSYD, March 31, 2022.\n",
    "reverse_repo_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/REVERSEREPO.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Gold Rate\n",
    "#### https://www.lbma.org.uk/prices-and-data/precious-metal-prices#/table\n",
    "gold_df = pd.read_csv('./inputFeatures/portfolioAllocations/commodities/gold.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### JPY Rate\n",
    "#### Board of Governors of the Federal Reserve System (US),\n",
    "#### Japanese Yen to U.S. Dollar Spot Exchange Rate [DEXJPUS],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/DEXJPUS, April 3, 2022.\n",
    "jpy_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/JPY.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "         \n",
    "eur_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/EUR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "gbp_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/GBP.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary functions\n",
    "def get_most_recent_value(date, lookup_df, column_name):\n",
    "    most_recent_date = [index for index in lookup_df.index if index <= date][-1]\n",
    "    return lookup_df[column_name].loc[most_recent_date]\n",
    "\n",
    "def calculate_percentage_change(index, number_of_days, lookup_df, column_name):\n",
    "    if (index + number_of_days < 0):\n",
    "        return None\n",
    "    if (index == combined_df.shape[0] - 1):\n",
    "        return None\n",
    "    final_value = lookup_df.iloc[max(index, index + number_of_days)][column_name]\n",
    "    starting_value = lookup_df.iloc[min(index, index + number_of_days)][column_name]\n",
    "    return (final_value - starting_value) / abs(starting_value) * 100\n",
    "\n",
    "dfs_to_combine = [\n",
    "    vix_df,\n",
    "    m1_df,\n",
    "    employment_df,\n",
    "    inflation_df,\n",
    "    gdp_df,\n",
    "    pcr_df,\n",
    "    umcsent_df,\n",
    "    confidence_df,\n",
    "    treasury_df,\n",
    "    effr_df,\n",
    "    repo_df,\n",
    "    reverse_repo_df,\n",
    "    gold_df,\n",
    "    jpy_df,\n",
    "    eur_df,\n",
    "    gbp_df]\n",
    "\n",
    "combined_df = spy_df\n",
    "combined_df = combined_df.join([df for df in dfs_to_combine])\n",
    "\n",
    "combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "\n",
    "for date in combined_df.index:\n",
    "    integer_location = combined_df.index.get_loc(date)\n",
    "\n",
    "    ### Fill in any gaps that may remain as a result of mismatched reported dates\n",
    "    ### This will repeat values in dates where there are extended periods without values\n",
    "    ### e.g. quarterly, monthly, weekly values\n",
    "    combined_df.at[date, 'M1Supply'] = get_most_recent_value(date, m1_df, 'M1Supply')\n",
    "    combined_df.at[date, 'EmploymentRate'] = get_most_recent_value(date, employment_df, 'EmploymentRate')\n",
    "    combined_df.at[date, 'InflationRate'] = get_most_recent_value(date, inflation_df, 'InflationRate')\n",
    "    combined_df.at[date, 'GDP'] = get_most_recent_value(date, gdp_df, 'GDP')\n",
    "    combined_df.at[date, 'UMCSENT'] = get_most_recent_value(date, umcsent_df, 'UMCSENT')\n",
    "    combined_df.at[date, 'Confidence'] = get_most_recent_value(date, confidence_df, 'Confidence')\n",
    "\n",
    "    ### Fill in gaps in treasury returns (due to bank holidays but market open)\n",
    "    ### by averaging the surrounding values if available\n",
    "    single_missing_value_columns = [\n",
    "        '1Mo', '3Mo', '1Yr', '2Yr', '5Yr', '10Yr', '20Yr', '30Yr',\n",
    "        'Repo', 'RepoRate', 'ReverseRepo', 'ReverseRepoRate', 'Price'\n",
    "    ]\n",
    "\n",
    "    for single_missing_value_column in single_missing_value_columns:\n",
    "        if (np.isnan(combined_df.at[date, single_missing_value_column])):\n",
    "            surrounding_values = combined_df.loc[\n",
    "                [combined_df.index[integer_location - 1], combined_df.index[integer_location + 1]],\n",
    "                single_missing_value_column\n",
    "            ].values\n",
    "            \n",
    "            if not (np.isnan(surrounding_values).any()):\n",
    "                combined_df.at[date, single_missing_value_column] = np.mean(surrounding_values)\n",
    "\n",
    "    combined_df['Repo'] = combined_df['Repo'].fillna(0.001)\n",
    "    combined_df['RepoRate'] = combined_df['RepoRate'].fillna(0.001)\n",
    "    combined_df['ReverseRepo'] = combined_df['ReverseRepo'].fillna(0.001)\n",
    "    combined_df['ReverseRepoRate'] = combined_df['ReverseRepoRate'].fillna(0.001)\n",
    "\n",
    "combined_df[combined_df < 0.001] = 0.001\n",
    "# combined_df.to_excel(\"./inputFeatures/combined.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "from time import time\n",
    "\n",
    "SEQ_LEN = 63\n",
    "FUTURE_PERIOD_PREDICT = 1\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # 'RepoRate', 'ReverseRepoRate', 'Repo', 'ReverseRepo', 'USDGBP', 'USDEUR', 'USDJPY', 'M1Supply', 'EmploymentRate', 'InflationRate', 'GDP', 'PCR', 'UMCSENT', 'Confidence', 'EFFR'\n",
    "    df = df.drop(columns=['future'])\n",
    "\n",
    "    output_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == 'SPYClose':\n",
    "            for number_of_days in [1, 2, 5, 21, 63]:\n",
    "                column_name = f'{col}{number_of_days}DayChange'\n",
    "                output_df = pd.concat([output_df, df[col].pct_change(number_of_days, fill_method='ffill').rename(column_name)], axis=1)\n",
    "                output_df.dropna(inplace=True)\n",
    "                output_df[column_name] = preprocessing.scale(output_df[column_name].values)\n",
    "                output_df[column_name] = output_df[column_name].clip(-3, 3) / 3\n",
    "        elif col in ['Volume', 'VIXClose', 'PCR', '1Mo', '3Mo', '1Yr', '2Yr', '5Yr', '10Yr', '20Yr', '30Yr', 'EFFR', 'Repo', 'RepoRate', 'ReverseRepo', 'ReverseRepoRate', 'Price', 'USDJPY', 'USDEUR', 'USDGBP']:\n",
    "            column_name = f'{col}DayChange'\n",
    "            output_df = pd.concat([output_df, df[col].pct_change(fill_method='ffill').rename(column_name)], axis=1)\n",
    "        elif col == 'M1Supply':\n",
    "            column_name = f'{col}WeekChange'\n",
    "            output_df = pd.concat([output_df, df[col].pct_change(7, fill_method='ffill').rename(column_name)], axis=1)\n",
    "        elif col in ['EmploymentRate', 'InflationRate', 'UMCSENT', 'Confidence']:\n",
    "            column_name = f'{col}MonthChange'\n",
    "            output_df = pd.concat([output_df, df[col].pct_change(24, fill_method='ffill').rename(column_name)], axis=1)\n",
    "        elif col == 'GDP':\n",
    "            column_name = f'{col}QuarterChange'\n",
    "            output_df = pd.concat([output_df, df[col].pct_change(65, fill_method='ffill').rename(column_name)], axis=1)\n",
    "        elif col == 'target':\n",
    "            output_df[col] = df[col]\n",
    "\n",
    "        if col not in ['SPYClose', 'target']:\n",
    "            output_df.dropna(inplace=True)\n",
    "            output_df[column_name] = preprocessing.scale(output_df[column_name].values)\n",
    "            output_df[column_name] = output_df[column_name].clip(-3, 3) / 3\n",
    "\n",
    "    output_df.dropna(inplace=True)\n",
    "\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in output_df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 21 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "            \n",
    "    np.random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    ups = []\n",
    "    downs = []\n",
    "\n",
    "    for sequence, target in sequential_data:    \n",
    "        if target == 0:\n",
    "            downs.append([sequence, target])\n",
    "        elif target == 1:\n",
    "            ups.append([sequence, target])\n",
    "    np.random.shuffle(ups)\n",
    "    np.random.shuffle(downs)\n",
    "\n",
    "    ## Get the value of the array with the smallest length\n",
    "    ## So we can ensure the training process is unbiased\n",
    "    ## As there will be 50:50 of up days and down days.\n",
    "    ## The model has to LEARN rather than REMEMBER\n",
    "    lower = min(len(ups), len(downs))\n",
    "\n",
    "    ups = ups[:lower]\n",
    "    downs = downs[:lower]\n",
    "\n",
    "    sequential_data = ups + downs\n",
    "\n",
    "    np.random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for sequence, target in sequential_data:\n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply preprocessing, arrange data (training, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for next day's closing price\n",
    "combined_df['future'] = combined_df['SPYClose'].shift(-1)\n",
    "# Add column to signify if next day's closing price is up (1) or down (0) using classify\n",
    "# function defined above\n",
    "combined_df['target'] = list(map(classify, combined_df['SPYClose'], combined_df['future']))\n",
    "\n",
    "\n",
    "times = sorted(combined_df.index.values)\n",
    "last_20pct = sorted(combined_df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times\n",
    "\n",
    "## Split in sample / out of sample\n",
    "validation_df = combined_df[(combined_df.index >= last_20pct)]  # make the validation data where the index is in the last 20%\n",
    "training_df = combined_df[(combined_df.index < last_20pct)]  # now the combined_df is all the data up to the last 20%\n",
    "\n",
    "train_x, train_y = preprocess_df(training_df)\n",
    "validation_x, validation_y = preprocess_df(validation_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {np.count_nonzero(train_y == 0)}, buys: {np.count_nonzero(train_y == 1)}\")\n",
    "print(f\"VALIDATION Dont buys: {np.count_nonzero(validation_y == 0)}, buys: {np.count_nonzero(validation_y == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, BatchNormalization, Dense, Conv1D, MaxPooling1D, Flatten, Concatenate\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract specific number of days for training from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_days(number_of_days, x):\n",
    "    return np.delete(x, np.s_[:-number_of_days], 1)\n",
    "\n",
    "# Use 21 days for initial tests\n",
    "initial_train_x = get_last_days(21, train_x)\n",
    "initial_validation_x = get_last_days(21, validation_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_1():\n",
    "    time_at_start = int(time())\n",
    "    lstm_layers = [1, 2]\n",
    "    dense_layers = [2, 3]\n",
    "    lstm_layer_sizes = [32, 64]\n",
    "    dense_layer_sizes = [32, 64]\n",
    "\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    for dense_layer in dense_layers:\n",
    "        for dense_layer_size in dense_layer_sizes:\n",
    "            for lstm_layer in lstm_layers:\n",
    "                for lstm_layer_size in lstm_layer_sizes:\n",
    "                    name = f\"{lstm_layer}LSTM{lstm_layer_size}-{dense_layer}DENSE{dense_layer_size}-{int(time())}\"\n",
    "\n",
    "                    model = Sequential()\n",
    "                    model.add(Input(shape=(initial_train_x.shape[1:])))\n",
    "\n",
    "                    for layer in range(lstm_layer - 1):            \n",
    "                        model.add(LSTM(lstm_layer_size, return_sequences=True))\n",
    "                        model.add(Dropout(0.4))\n",
    "                        model.add(BatchNormalization())\n",
    "\n",
    "                    model.add(LSTM(lstm_layer_size))\n",
    "                    model.add(Dropout(0.4))\n",
    "                    model.add(BatchNormalization())\n",
    "\n",
    "                    for layer in range(dense_layer - 1):\n",
    "                        model.add(Dense(dense_layer_size, activation='relu'))\n",
    "                        model.add(Dropout(0.4))\n",
    "\n",
    "                    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "                    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                    optimizer=opt,\n",
    "                                    metrics=['accuracy'])\n",
    "\n",
    "                    tensorboard = TensorBoard(log_dir=f'lstmiteration1logs-{time_at_start}/{name}')\n",
    "\n",
    "                    checkpoint_filepath = f\"lstmiteration1models-{time_at_start}/\" + name + \"-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "                    checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "                    early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=12)\n",
    "\n",
    "                    history = model.fit(\n",
    "                        initial_train_x, train_y,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=(initial_validation_x, validation_y),\n",
    "                        callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2 - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last(x):\n",
    "    return x[-1]\n",
    "\n",
    "number_of_features = train_x.shape[-1]\n",
    "cnn_train_x = get_last_days(1, train_x)\n",
    "cnn_validation_x = get_last_days(1, validation_x)\n",
    "\n",
    "def iteration_2():\n",
    "    time_at_start = int(time())\n",
    "    conv_layers = [1, 2]\n",
    "    conv_layer_sizes = [16, 32]\n",
    "    dense_layers = [1, 2]\n",
    "    dense_layer_sizes = [32, 64]\n",
    "\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    for dense_layer in dense_layers:\n",
    "        for dense_layer_size in dense_layer_sizes:\n",
    "                    for conv_layer in conv_layers:\n",
    "                        for conv_layer_size in conv_layer_sizes:\n",
    "                            name = f\"{conv_layer}C{conv_layer_size}-{dense_layer}D{dense_layer_size}-{int(time())}\"\n",
    "\n",
    "                            model = Sequential()\n",
    "                            model.add(Input(shape=(cnn_train_x.shape[1:])))\n",
    "\n",
    "                            for layer in range(conv_layer - 1):\n",
    "                                model.add(Conv1D(conv_layer_size, kernel_size=1, padding='valid', activation='relu'))\n",
    "                                model.add(MaxPooling1D(1))\n",
    "                    \n",
    "                            model.add(Conv1D(conv_layer_size, kernel_size=1, padding='valid', activation='relu'))\n",
    "                            model.add(MaxPooling1D(1))\n",
    "\n",
    "                            model.add(Flatten())\n",
    "\n",
    "                            for layer in range(dense_layer - 1):\n",
    "                                model.add(Dense(dense_layer_size, activation='relu'))\n",
    "                                model.add(Dropout(0.4))\n",
    "\n",
    "                            model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "                            opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "                            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                            optimizer=opt,\n",
    "                                            metrics=['accuracy'])\n",
    "\n",
    "                            tensorboard = TensorBoard(log_dir=f'cnniteration2logs-{time_at_start}/{name}')\n",
    "\n",
    "                            checkpoint_filepath = f\"cnniteration2models-{time_at_start}/\" + name + \"-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "                            checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "                            early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=12)\n",
    "\n",
    "                            history = model.fit(\n",
    "                                cnn_train_x, train_y,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=(cnn_validation_x, validation_y),\n",
    "                                callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3 - CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_3():\n",
    "    time_at_start = int(time())\n",
    "    conv_layers = [1,2]\n",
    "    conv_layer_sizes = [16, 32]\n",
    "    lstm_layers = [1,2]\n",
    "    dense_layers = [2,3]\n",
    "    lstm_layer_sizes = [16, 32]\n",
    "    dense_layer_sizes = [32, 64]\n",
    "\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    for dense_layer in dense_layers:\n",
    "        for dense_layer_size in dense_layer_sizes:\n",
    "            for lstm_layer in lstm_layers:\n",
    "                for lstm_layer_size in lstm_layer_sizes:\n",
    "                    for conv_layer in conv_layers:\n",
    "                        for conv_layer_size in conv_layer_sizes:\n",
    "                            name = f\"{conv_layer}C{conv_layer_size}-{lstm_layer}L{lstm_layer_size}-{dense_layer}D{dense_layer_size}-{int(time())}\"\n",
    "\n",
    "                            model = Sequential()\n",
    "                            model.add(Input(shape=(initial_train_x.shape[1:])))\n",
    "                            \n",
    "                            for layer in range(conv_layer - 1):            \n",
    "                                model.add(Conv1D(conv_layer_size, 2, padding='same'))\n",
    "                                \n",
    "                            model.add(Conv1D(conv_layer_size, 2, padding='same'))\n",
    "\n",
    "                            for layer in range(lstm_layer - 1):            \n",
    "                                model.add(LSTM(lstm_layer_size, return_sequences=True))\n",
    "                                model.add(Dropout(0.4))\n",
    "                                model.add(BatchNormalization())\n",
    "\n",
    "                            model.add(LSTM(lstm_layer_size))\n",
    "                            model.add(Dropout(0.4))\n",
    "                            model.add(BatchNormalization())\n",
    "\n",
    "                            for layer in range(dense_layer - 1):\n",
    "                                model.add(Dense(dense_layer_size, activation='relu'))\n",
    "                                model.add(Dropout(0.4))\n",
    "\n",
    "                            model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "                            opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "                            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                            optimizer=opt,\n",
    "                                            metrics=['accuracy'])\n",
    "\n",
    "                            tensorboard = TensorBoard(log_dir=f'cnnlstmiteration3logs-{time_at_start}/{name}')\n",
    "\n",
    "                            checkpoint_filepath = f\"cnnlstmiteration3models-{time_at_start}/\" + name + \"-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "                            checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "                            early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=12)\n",
    "\n",
    "                            history = model.fit(\n",
    "                                initial_train_x, train_y,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=(initial_validation_x, validation_y),\n",
    "                                callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4 - LSTM + CNN + CNNLSTM (concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_4():\n",
    "    # LSTM\n",
    "    lstm_input = Input(shape=(initial_train_x.shape[1:]))\n",
    "        \n",
    "    lstm_lstm_1 = LSTM(64, return_sequences=True)(lstm_input)\n",
    "    lstm_dropout_1 = Dropout(0.4)(lstm_lstm_1)\n",
    "    lstm_bn_1 = BatchNormalization()(lstm_dropout_1)\n",
    "\n",
    "    lstm_lstm_2 = LSTM(64)(lstm_bn_1)\n",
    "    lstm_dropout_2 = Dropout(0.4)(lstm_lstm_2)\n",
    "    lstm_bn_2 = BatchNormalization()(lstm_dropout_2)\n",
    "\n",
    "    lstm_dense_1 = Dense(32, activation='relu')(lstm_bn_2)\n",
    "    lstm_dropout_3 = Dropout(0.4)(lstm_dense_1)\n",
    "\n",
    "    # # lstm_dense_3 = Dense(2, activation='softmax')(lstm_dropout_3)\n",
    "\n",
    "    # CNN\n",
    "    cnn_input = Input(shape=(cnn_train_x.shape[1:]))\n",
    "\n",
    "    cnn_conv_1 = Conv1D(32, kernel_size=1, padding='valid', activation='relu')(cnn_input)\n",
    "    cnn_pool_1 = MaxPooling1D(1)(cnn_conv_1)\n",
    "    cnn_flat_1 = Flatten()(cnn_pool_1)\n",
    "\n",
    "    # # cnn_dense_1 = Dense(2, activation='softmax')(cnn_flat_1)\n",
    "\n",
    "    # CNN LSTM\n",
    "    cnn_lstm_input = Input(shape=(initial_train_x.shape[1:]))\n",
    "                    \n",
    "    cnn_lstm_conv_1 = Conv1D(32, 2, padding='same')(cnn_lstm_input)\n",
    "    cnn_lstm_conv_2 = Conv1D(32, 2, padding='same')(cnn_lstm_conv_1)\n",
    "\n",
    "    cnn_lstm_lstm_1 = LSTM(16)(cnn_lstm_conv_2)\n",
    "    cnn_lstm_drop_1 = Dropout(0.4)(cnn_lstm_lstm_1)\n",
    "    cnn_lstm_bn_1 = BatchNormalization()(cnn_lstm_drop_1)\n",
    "\n",
    "    cnn_lstm_dense_1 = Dense(64, activation='relu')(cnn_lstm_bn_1)\n",
    "    cnn_lstm_drop_2 = Dropout(0.4)(cnn_lstm_dense_1)\n",
    "\n",
    "    # # cnn_lstm_dense_2 = Dense(2, activation='softmax')(cnn_lstm_drop_2)\n",
    "\n",
    "    concat = Concatenate()([lstm_dropout_3, cnn_flat_1, cnn_lstm_drop_2])\n",
    "    final_dense_1 = Dense(32, activation='relu')(concat)\n",
    "    output = Dense(2, activation='softmax')(final_dense_1)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[lstm_input, cnn_input, cnn_lstm_input], outputs=[output])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                    optimizer=opt,\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=f'ensemble1logs-{int(time())}/Ensemble-Model')\n",
    "\n",
    "    checkpoint_filepath = f\"ensemble1models-{int(time())}\" + \"/Ensemble-Model-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "    checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=12)\n",
    "\n",
    "    history = model.fit(\n",
    "        [initial_train_x, cnn_train_x, initial_train_x], train_y,\n",
    "        batch_size=32,\n",
    "        epochs=30,\n",
    "        validation_data=([initial_validation_x, cnn_validation_x, initial_validation_x], validation_y),\n",
    "        callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "    )\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 5 - Sequence Lengths + Input Combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_two_days = get_last_days(2, train_x) # 2 days\n",
    "train_x_week = get_last_days(5, train_x) # 1 week\n",
    "train_x_month = get_last_days(21, train_x) # 1 month\n",
    "train_x # 1 quarter\n",
    "\n",
    "validation_x_two_days = get_last_days(2, validation_x) # 2 days\n",
    "validation_x_week = get_last_days(5, validation_x) # 1 week\n",
    "validation_x_month = get_last_days(21, validation_x) # 1 month\n",
    "validation_x # 1 quarter\n",
    "\n",
    "def extract_columns(columns_array, x):\n",
    "    return x[:,:,columns_array]\n",
    "\n",
    "def iteration_5_model(name, time_at_start, train_inputs, validation_inputs):\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(train_inputs.shape[1:])))\n",
    "        \n",
    "    model.add(Conv1D(32, 2, padding='same'))\n",
    "    model.add(Conv1D(32, 2, padding='same'))\n",
    "\n",
    "    model.add(LSTM(16))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                    optimizer=opt,\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=f'cnnlstmiteration5logs-{time_at_start}/{name}')\n",
    "\n",
    "    checkpoint_filepath = f\"cnnlstmiteration5models-{time_at_start}/\" + name + \"-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "    checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=12)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_inputs, train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(validation_inputs, validation_y),\n",
    "        callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "    )\n",
    "\n",
    "def iteration_5():\n",
    "    time_at_start = int(time())\n",
    "\n",
    "    for training_set in ['two_days', 'week', 'month', 'quarter']:\n",
    "        if training_set == 'two_days':\n",
    "            training_set_to_use = train_x_two_days\n",
    "            validation_set_to_use = validation_x_two_days\n",
    "        elif training_set == 'week':\n",
    "            training_set_to_use = train_x_week\n",
    "            validation_set_to_use = validation_x_week\n",
    "        elif training_set == 'month':\n",
    "            training_set_to_use = train_x_month\n",
    "            validation_set_to_use = validation_x_month\n",
    "        elif training_set == 'quarter':\n",
    "            training_set_to_use = train_x\n",
    "            validation_set_to_use = validation_x\n",
    "        for input_features in ['price_only', 'price_extended_and_volume_and_volatility',\n",
    "        'price_and_money_supply_and_gdp', 'price_and_treasury_yields',\n",
    "        'price_and_effr_and_repo', 'price_and_gold_and_currency', 'price_and_options',\n",
    "        'price_and_inflation', 'price_and_employment', 'price_and_other_sentiment']:\n",
    "            if input_features == 'price_only':\n",
    "                train_inputs = extract_columns([0], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0], validation_set_to_use)\n",
    "            elif input_features == 'price_extended_and_volume_and_volatility':\n",
    "                train_inputs = extract_columns([0, 1, 2, 3, 4, 5, 6], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 1, 2, 3, 4, 5, 6], validation_set_to_use)\n",
    "            elif input_features == 'price_and_money_supply_and_gdp':\n",
    "                train_inputs = extract_columns([0, 7, 8], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 7, 8], validation_set_to_use)\n",
    "            elif input_features == 'price_and_treasury_yields':\n",
    "                train_inputs = extract_columns([0, 14, 15, 16, 17, 17, 19, 20, 21], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 14, 15, 16, 17, 17, 19, 20, 21], validation_set_to_use)\n",
    "            elif input_features == 'price_and_effr_and_repo':\n",
    "                train_inputs = extract_columns([0, 22, 23, 24, 25, 26], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 22, 23, 24, 25, 26], validation_set_to_use)\n",
    "            elif input_features == 'price_and_gold_and_currency':\n",
    "                train_inputs = extract_columns([0, 27, 28, 29, 30], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 27, 28, 29, 30], validation_set_to_use)\n",
    "            elif input_features == 'price_and_options':\n",
    "                train_inputs = extract_columns([0, 11], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 11], validation_set_to_use)\n",
    "            elif input_features == 'price_and_inflation':\n",
    "                train_inputs = extract_columns([0, 9], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 9], validation_set_to_use)\n",
    "            elif input_features == 'price_and_employment':\n",
    "                train_inputs = extract_columns([0, 8], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 8], validation_set_to_use)\n",
    "            elif input_features == 'price_and_other_sentiment':\n",
    "                train_inputs = extract_columns([0, 12, 13], training_set_to_use)\n",
    "                validation_inputs = extract_columns([0, 12, 13], validation_set_to_use)\n",
    "            \n",
    "            name = f\"{training_set}-{input_features}-{int(time())}\"\n",
    "            iteration_5_model(name, time_at_start, train_inputs, validation_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_days-price_only\n",
      "(2730, 2, 1) (2730,)\n",
      "(560, 2, 1) (560,)\n",
      "two_days-price_extended_and_volume_and_volatility\n",
      "(2730, 2, 7) (2730,)\n",
      "(560, 2, 7) (560,)\n",
      "two_days-price_and_money_supply_and_gdp\n",
      "(2730, 2, 3) (2730,)\n",
      "(560, 2, 3) (560,)\n",
      "two_days-price_and_treasury_yields\n",
      "(2730, 2, 9) (2730,)\n",
      "(560, 2, 9) (560,)\n",
      "two_days-price_and_effr_and_repo\n",
      "(2730, 2, 6) (2730,)\n",
      "(560, 2, 6) (560,)\n",
      "two_days-price_and_gold_and_currency\n",
      "(2730, 2, 5) (2730,)\n",
      "(560, 2, 5) (560,)\n",
      "two_days-price_and_options\n",
      "(2730, 2, 2) (2730,)\n",
      "(560, 2, 2) (560,)\n",
      "two_days-price_and_inflation\n",
      "(2730, 2, 2) (2730,)\n",
      "(560, 2, 2) (560,)\n",
      "two_days-price_and_employment\n",
      "(2730, 2, 2) (2730,)\n",
      "(560, 2, 2) (560,)\n",
      "two_days-price_and_other_sentiment\n",
      "(2730, 2, 3) (2730,)\n",
      "(560, 2, 3) (560,)\n",
      "week-price_only\n",
      "(2730, 5, 1) (2730,)\n",
      "(560, 5, 1) (560,)\n",
      "week-price_extended_and_volume_and_volatility\n",
      "(2730, 5, 7) (2730,)\n",
      "(560, 5, 7) (560,)\n",
      "week-price_and_money_supply_and_gdp\n",
      "(2730, 5, 3) (2730,)\n",
      "(560, 5, 3) (560,)\n",
      "week-price_and_treasury_yields\n",
      "(2730, 5, 9) (2730,)\n",
      "(560, 5, 9) (560,)\n",
      "week-price_and_effr_and_repo\n",
      "(2730, 5, 6) (2730,)\n",
      "(560, 5, 6) (560,)\n",
      "week-price_and_gold_and_currency\n",
      "(2730, 5, 5) (2730,)\n",
      "(560, 5, 5) (560,)\n",
      "week-price_and_options\n",
      "(2730, 5, 2) (2730,)\n",
      "(560, 5, 2) (560,)\n",
      "week-price_and_inflation\n",
      "(2730, 5, 2) (2730,)\n",
      "(560, 5, 2) (560,)\n",
      "week-price_and_employment\n",
      "(2730, 5, 2) (2730,)\n",
      "(560, 5, 2) (560,)\n",
      "week-price_and_other_sentiment\n",
      "(2730, 5, 3) (2730,)\n",
      "(560, 5, 3) (560,)\n",
      "month-price_only\n",
      "(2730, 21, 1) (2730,)\n",
      "(560, 21, 1) (560,)\n",
      "month-price_extended_and_volume_and_volatility\n",
      "(2730, 21, 7) (2730,)\n",
      "(560, 21, 7) (560,)\n",
      "month-price_and_money_supply_and_gdp\n",
      "(2730, 21, 3) (2730,)\n",
      "(560, 21, 3) (560,)\n",
      "month-price_and_treasury_yields\n",
      "(2730, 21, 9) (2730,)\n",
      "(560, 21, 9) (560,)\n",
      "month-price_and_effr_and_repo\n",
      "(2730, 21, 6) (2730,)\n",
      "(560, 21, 6) (560,)\n",
      "month-price_and_gold_and_currency\n",
      "(2730, 21, 5) (2730,)\n",
      "(560, 21, 5) (560,)\n",
      "month-price_and_options\n",
      "(2730, 21, 2) (2730,)\n",
      "(560, 21, 2) (560,)\n",
      "month-price_and_inflation\n",
      "(2730, 21, 2) (2730,)\n",
      "(560, 21, 2) (560,)\n",
      "month-price_and_employment\n",
      "(2730, 21, 2) (2730,)\n",
      "(560, 21, 2) (560,)\n",
      "month-price_and_other_sentiment\n",
      "(2730, 21, 3) (2730,)\n",
      "(560, 21, 3) (560,)\n",
      "quarter-price_only\n",
      "(2730, 63, 1) (2730,)\n",
      "(560, 63, 1) (560,)\n",
      "quarter-price_extended_and_volume_and_volatility\n",
      "(2730, 63, 7) (2730,)\n",
      "(560, 63, 7) (560,)\n",
      "quarter-price_and_money_supply_and_gdp\n",
      "(2730, 63, 3) (2730,)\n",
      "(560, 63, 3) (560,)\n",
      "quarter-price_and_treasury_yields\n",
      "(2730, 63, 9) (2730,)\n",
      "(560, 63, 9) (560,)\n",
      "quarter-price_and_effr_and_repo\n",
      "(2730, 63, 6) (2730,)\n",
      "(560, 63, 6) (560,)\n",
      "quarter-price_and_gold_and_currency\n",
      "(2730, 63, 5) (2730,)\n",
      "(560, 63, 5) (560,)\n",
      "quarter-price_and_options\n",
      "(2730, 63, 2) (2730,)\n",
      "(560, 63, 2) (560,)\n",
      "quarter-price_and_inflation\n",
      "(2730, 63, 2) (2730,)\n",
      "(560, 63, 2) (560,)\n",
      "quarter-price_and_employment\n",
      "(2730, 63, 2) (2730,)\n",
      "(560, 63, 2) (560,)\n",
      "quarter-price_and_other_sentiment\n",
      "(2730, 63, 3) (2730,)\n",
      "(560, 63, 3) (560,)\n"
     ]
    }
   ],
   "source": [
    "iteration_5()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
