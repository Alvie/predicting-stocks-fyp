{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform inputs to pandas dataframe\n",
    "\n",
    "## Stock Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPY Price & Volume\n",
    "#### https://uk.finance.yahoo.com/quote/SPY/history\n",
    "spy_df = pd.read_csv('./inputFeatures/stockIndex/SPY.csv',\n",
    "    index_col=[\"Date\"], \n",
    "    usecols=[\"Date\", \"SPYClose\", \"Volume\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### VIX (Volatility Index)\n",
    "#### https://uk.finance.yahoo.com/quote/%5EVIX/history\n",
    "vix_df = pd.read_csv('./inputFeatures/stockIndex/VIX.csv',\n",
    "    index_col=[\"Date\"], \n",
    "    usecols=[\"Date\", \"VIXClose\"],\n",
    "    parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### M1 Money Supply\n",
    "#### Board of Governors of the Federal Reserve System (US), M1 [WM1NS],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/WM1NS, March 31, 2022.\n",
    "m1_df = pd.read_csv('./inputFeatures/moneyAvailability/WM1NS.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### Employment Rate\n",
    "#### Organization for Economic Co-operation and Development,\n",
    "#### Employment Rate: Aged 15-64: All Persons for the United States\n",
    "#### [LREM64TTUSM156S], retrieved from FRED,\n",
    "#### Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/LREM64TTUSM156S, March 31, 2022.\n",
    "employment_df = pd.read_csv('./inputFeatures/moneyAvailability/EmploymentRate.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### Inflation Rate\n",
    "## data.bls.gov\n",
    "inflation_df = pd.read_csv('./inputFeatures/moneyAvailability/InflationRate.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### GDP Rate\n",
    "#### U.S. Bureau of Economic Analysis, Gross Domestic Product [GDP], retrieved from FRED,\n",
    "#### Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/GDP, March 31, 2022.\n",
    "gdp_df = pd.read_csv('./inputFeatures/moneyAvailability/GDP.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put Call Ratio\n",
    "#### https://www.alphalerts.com/live-historical-equity-pcr/\n",
    "pcr_df = pd.read_csv('./inputFeatures/sentimentIndicators/PCR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Consumer Sentiment\n",
    "#### Surveys of Consumers, University of Michigan: Consumer Sentiment Â© [UMCSENT]\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/UMCSENT, March 31, 2022.\n",
    "umcsent_df = pd.read_csv('./inputFeatures/sentimentIndicators/UMCSENT.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "confidence_df = pd.read_csv('./inputFeatures/sentimentIndicators/CSCICP03USM665S.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Treasury Yield Rates\n",
    "#### https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve\n",
    "treasury_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/daily-treasury-rates.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Effective Funds Rate\n",
    "#### Federal Reserve Bank of New York, Effective Federal Funds Rate [EFFR],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/EFFR, March 31, 2022.\n",
    "effr_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/EFFR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Accepted Repurchase Agreements (Repo) by the Federal Reserve\n",
    "#### Federal Reserve Bank of New York, Overnight Repurchase Agreements:\n",
    "#### Treasury Securities Purchased by the Federal Reserve in the Temporary Open Market Operations [RPONTSYD],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/RPONTSYD, March 31, 2022.\n",
    "#### https://www.newyorkfed.org/markets/desk-operations/repo\n",
    "repo_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/REPO.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Accepted Reverse Repurchase Agreements (Reverse Repo) by the Federal Reserve\n",
    "#### Federal Reserve Bank of New York, Overnight Reverse Repurchase Agreements:\n",
    "#### Treasury Securities Sold by the Federal Reserve in the Temporary Open Market Operations [RRPONTSYD],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/RRPONTSYD, March 31, 2022.\n",
    "reverse_repo_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/REVERSEREPO.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Gold Rate\n",
    "#### https://www.lbma.org.uk/prices-and-data/precious-metal-prices#/table\n",
    "gold_df = pd.read_csv('./inputFeatures/portfolioAllocations/commodities/gold.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### JPY Rate\n",
    "#### Board of Governors of the Federal Reserve System (US),\n",
    "#### Japanese Yen to U.S. Dollar Spot Exchange Rate [DEXJPUS],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/DEXJPUS, April 3, 2022.\n",
    "jpy_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/JPY.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "         \n",
    "eur_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/EUR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "gbp_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/GBP.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary functions\n",
    "def get_most_recent_value(date, lookup_df, column_name):\n",
    "    most_recent_date = [index for index in lookup_df.index if index <= date][-1]\n",
    "    return lookup_df[column_name].loc[most_recent_date]\n",
    "\n",
    "def calculate_percentage_change(index, number_of_days, lookup_df, column_name):\n",
    "    if (index + number_of_days < 0):\n",
    "        return None\n",
    "    if (index == combined_df.shape[0] - 1):\n",
    "        return None\n",
    "    final_value = lookup_df.iloc[max(index, index + number_of_days)][column_name]\n",
    "    starting_value = lookup_df.iloc[min(index, index + number_of_days)][column_name]\n",
    "    return (final_value - starting_value) / abs(starting_value) * 100\n",
    "\n",
    "dfs_to_combine = [\n",
    "    vix_df,\n",
    "    m1_df,\n",
    "    employment_df,\n",
    "    inflation_df,\n",
    "    gdp_df,\n",
    "    pcr_df,\n",
    "    umcsent_df,\n",
    "    confidence_df,\n",
    "    treasury_df,\n",
    "    effr_df,\n",
    "    repo_df,\n",
    "    reverse_repo_df,\n",
    "    gold_df,\n",
    "    jpy_df,\n",
    "    eur_df,\n",
    "    gbp_df]\n",
    "\n",
    "combined_df = spy_df\n",
    "combined_df = combined_df.join([df for df in dfs_to_combine])\n",
    "\n",
    "combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "\n",
    "for date in combined_df.index:\n",
    "    integer_location = combined_df.index.get_loc(date)\n",
    "\n",
    "    ### Fill in any gaps that may remain as a result of mismatched reported dates\n",
    "    ### This will repeat values in dates where there are extended periods without values\n",
    "    ### e.g. quarterly, monthly, weekly values\n",
    "    combined_df.at[date, 'M1Supply'] = get_most_recent_value(date, m1_df, 'M1Supply')\n",
    "    combined_df.at[date, 'EmploymentRate'] = get_most_recent_value(date, employment_df, 'EmploymentRate')\n",
    "    combined_df.at[date, 'InflationRate'] = get_most_recent_value(date, inflation_df, 'InflationRate')\n",
    "    combined_df.at[date, 'GDP'] = get_most_recent_value(date, gdp_df, 'GDP')\n",
    "    combined_df.at[date, 'UMCSENT'] = get_most_recent_value(date, umcsent_df, 'UMCSENT')\n",
    "    combined_df.at[date, 'Confidence'] = get_most_recent_value(date, confidence_df, 'Confidence')\n",
    "\n",
    "    ### Fill in gaps in treasury returns (due to bank holidays but market open)\n",
    "    ### by averaging the surrounding values if available\n",
    "    single_missing_value_columns = [\n",
    "        '1Mo', '3Mo', '1Yr', '2Yr', '5Yr', '10Yr', '20Yr', '30Yr',\n",
    "        'Repo', 'RepoRate', 'ReverseRepo', 'ReverseRepoRate', 'Price'\n",
    "    ]\n",
    "\n",
    "    for single_missing_value_column in single_missing_value_columns:\n",
    "        if (np.isnan(combined_df.at[date, single_missing_value_column])):\n",
    "            surrounding_values = combined_df.loc[\n",
    "                [combined_df.index[integer_location - 1], combined_df.index[integer_location + 1]],\n",
    "                single_missing_value_column\n",
    "            ].values\n",
    "            \n",
    "            if not (np.isnan(surrounding_values).any()):\n",
    "                combined_df.at[date, single_missing_value_column] = np.mean(surrounding_values)\n",
    "\n",
    "    combined_df['Repo'] = combined_df['Repo'].fillna(0.001)\n",
    "    combined_df['RepoRate'] = combined_df['RepoRate'].fillna(0.001)\n",
    "    combined_df['ReverseRepo'] = combined_df['ReverseRepo'].fillna(0.001)\n",
    "    combined_df['ReverseRepoRate'] = combined_df['ReverseRepoRate'].fillna(0.001)\n",
    "\n",
    "combined_df[combined_df < 0.001] = 0.001\n",
    "# combined_df.to_excel(\"./inputFeatures/combined.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "from time import time\n",
    "\n",
    "SEQ_LEN = 21\n",
    "FUTURE_PERIOD_PREDICT = 1\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # 'RepoRate', 'ReverseRepoRate', 'Repo', 'ReverseRepo', 'USDGBP', 'USDEUR', 'USDJPY', 'M1Supply', 'EmploymentRate', 'InflationRate', 'GDP', 'PCR', 'UMCSENT', 'Confidence', 'EFFR'\n",
    "    df = df.drop(columns=['future'])\n",
    "\n",
    "    output_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in ['SPYClose', 'Volume', 'VIXClose', 'PCR']:\n",
    "            for number_of_days in [1]:\n",
    "                column_name = f'{col}{number_of_days}DaysChange'\n",
    "                output_df = pd.concat([output_df, df[col].pct_change(number_of_days, fill_method='ffill').rename(column_name)], axis=1)\n",
    "                output_df.dropna(inplace=True)\n",
    "                output_df[column_name] = preprocessing.scale(output_df[column_name].values)\n",
    "        elif col == 'target':\n",
    "            output_df[col] = df[col]\n",
    "\n",
    "    output_df.dropna(inplace=True)\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in output_df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 21 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "            \n",
    "    np.random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    ups = []\n",
    "    downs = []\n",
    "\n",
    "    for sequence, target in sequential_data:    \n",
    "        if target == 0:\n",
    "            downs.append([sequence, target])\n",
    "        elif target == 1:\n",
    "            ups.append([sequence, target])\n",
    "    np.random.shuffle(ups)\n",
    "    np.random.shuffle(downs)\n",
    "\n",
    "    ## Get the value of the array with the smallest length\n",
    "    ## So we can ensure the training process is unbiased\n",
    "    ## As there will be 50:50 of up days and down days.\n",
    "    ## The model has to LEARN rather than REMEMBER\n",
    "    lower = min(len(ups), len(downs))\n",
    "\n",
    "    ups = ups[:lower]\n",
    "    downs = downs[:lower]\n",
    "\n",
    "    sequential_data = ups + downs\n",
    "\n",
    "    np.random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for sequence, target in sequential_data:\n",
    "        X.append(sequence)\n",
    "        y.append(target)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply preprocessing, arrange data (training, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for next day's closing price\n",
    "combined_df['future'] = combined_df['SPYClose'].shift(-1)\n",
    "# Add column to signify if next day's closing price is up (1) or down (0) using classify\n",
    "# function defined above\n",
    "combined_df['target'] = list(map(classify, combined_df['SPYClose'], combined_df['future']))\n",
    "\n",
    "\n",
    "times = sorted(combined_df.index.values)\n",
    "last_20pct = sorted(combined_df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times\n",
    "\n",
    "## Split in sample / out of sample\n",
    "validation_df = combined_df[(combined_df.index >= last_20pct)]  # make the validation data where the index is in the last 20%\n",
    "training_df = combined_df[(combined_df.index < last_20pct)]  # now the combined_df is all the data up to the last 20%\n",
    "\n",
    "train_x, train_y = preprocess_df(training_df)\n",
    "validation_x, validation_y = preprocess_df(validation_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {np.count_nonzero(train_y == 0)}, buys: {np.count_nonzero(train_y == 1)}\")\n",
    "print(f\"VALIDATION Dont buys: {np.count_nonzero(validation_y == 0)}, buys: {np.count_nonzero(validation_y == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, BatchNormalization, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_1():\n",
    "    lstm_layers = [3]\n",
    "    dense_layers = [1, 2]\n",
    "    lstm_layer_sizes = [64]\n",
    "    dense_layer_sizes = [16, 32, 64, 128]\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    for dense_layer in dense_layers:\n",
    "        for dense_layer_size in dense_layer_sizes:\n",
    "            for lstm_layer in lstm_layers:\n",
    "                for lstm_layer_size in lstm_layer_sizes:\n",
    "                    name = f\"SVVO-{lstm_layer}LSTM{lstm_layer_size}-{dense_layer}DENSE{dense_layer_size}-{int(time())}\"\n",
    "\n",
    "                    model = Sequential()\n",
    "                    model.add(Input(shape=(train_x.shape[1:])))\n",
    "\n",
    "                    for layer in range(lstm_layer - 1):            \n",
    "                        model.add(LSTM(lstm_layer_size, return_sequences=True))\n",
    "                        model.add(Dropout(0.2))\n",
    "                        model.add(BatchNormalization())\n",
    "\n",
    "                    model.add(LSTM(lstm_layer_size))\n",
    "                    model.add(Dropout(0.2))\n",
    "                    model.add(BatchNormalization())\n",
    "\n",
    "                    for layer in range(dense_layer - 1):\n",
    "                        model.add(Dense(dense_layer_size, activation='relu'))\n",
    "                        model.add(Dropout(0.2))\n",
    "\n",
    "                    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "                    opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "                    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                    optimizer=opt,\n",
    "                                    metrics=['accuracy'])\n",
    "\n",
    "                    tensorboard = TensorBoard(log_dir=f'iteration1logs/{name}')\n",
    "\n",
    "                    checkpoint_filepath = \"iteration1models/\" + name + \"-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "                    checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "                    early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=8)\n",
    "\n",
    "                    history = model.fit(\n",
    "                        train_x, train_y,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=(validation_x, validation_y),\n",
    "                        callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "                    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
