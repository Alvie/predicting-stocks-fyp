{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform inputs to pandas dataframe\n",
    "\n",
    "## Stock Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPY Price & Volume\n",
    "#### https://uk.finance.yahoo.com/quote/SPY/history\n",
    "spy_df = pd.read_csv('./inputFeatures/stockIndex/SPY.csv',\n",
    "    index_col=[\"Date\"], \n",
    "    usecols=[\"Date\", \"SPYClose\", \"Volume\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### VIX (Volatility Index)\n",
    "#### https://uk.finance.yahoo.com/quote/%5EVIX/history\n",
    "vix_df = pd.read_csv('./inputFeatures/stockIndex/VIX.csv',\n",
    "    index_col=[\"Date\"], \n",
    "    usecols=[\"Date\", \"VIXClose\"],\n",
    "    parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Money availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### M1 Money Supply\n",
    "#### Board of Governors of the Federal Reserve System (US), M1 [WM1NS],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/WM1NS, March 31, 2022.\n",
    "m1_df = pd.read_csv('./inputFeatures/moneyAvailability/WM1NS.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### Employment Rate\n",
    "#### Organization for Economic Co-operation and Development,\n",
    "#### Employment Rate: Aged 15-64: All Persons for the United States\n",
    "#### [LREM64TTUSM156S], retrieved from FRED,\n",
    "#### Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/LREM64TTUSM156S, March 31, 2022.\n",
    "employment_df = pd.read_csv('./inputFeatures/moneyAvailability/EmploymentRate.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### Inflation Rate\n",
    "## data.bls.gov\n",
    "inflation_df = pd.read_csv('./inputFeatures/moneyAvailability/InflationRate.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])\n",
    "\n",
    "### GDP Rate\n",
    "#### U.S. Bureau of Economic Analysis, Gross Domestic Product [GDP], retrieved from FRED,\n",
    "#### Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/GDP, March 31, 2022.\n",
    "gdp_df = pd.read_csv('./inputFeatures/moneyAvailability/GDP.csv',\n",
    "    index_col=[\"Date\"],\n",
    "    parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put Call Ratio\n",
    "#### https://www.alphalerts.com/live-historical-equity-pcr/\n",
    "pcr_df = pd.read_csv('./inputFeatures/sentimentIndicators/PCR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Consumer Sentiment\n",
    "#### Surveys of Consumers, University of Michigan: Consumer Sentiment Â© [UMCSENT]\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/UMCSENT, March 31, 2022.\n",
    "umcsent_df = pd.read_csv('./inputFeatures/sentimentIndicators/UMCSENT.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "confidence_df = pd.read_csv('./inputFeatures/sentimentIndicators/CSCICP03USM665S.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Treasury Yield Rates\n",
    "#### https://www.alphalerts.com/live-historical-equity-pcr/\n",
    "treasury_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/daily-treasury-rates.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Effective Funds Rate\n",
    "#### Federal Reserve Bank of New York, Effective Federal Funds Rate [EFFR],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/EFFR, March 31, 2022.\n",
    "effr_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/EFFR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Accepted Repurchase Agreements (Repo) by the Federal Reserve\n",
    "#### Federal Reserve Bank of New York, Overnight Repurchase Agreements:\n",
    "#### Treasury Securities Purchased by the Federal Reserve in the Temporary Open Market Operations [RPONTSYD],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/RPONTSYD, March 31, 2022.\n",
    "repo_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/REPO.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Accepted Reverse Repurchase Agreements (Reverse Repo) by the Federal Reserve\n",
    "#### Federal Reserve Bank of New York, Overnight Reverse Repurchase Agreements:\n",
    "#### Treasury Securities Sold by the Federal Reserve in the Temporary Open Market Operations [RRPONTSYD],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/RRPONTSYD, March 31, 2022.\n",
    "reverse_repo_df = pd.read_csv('./inputFeatures/portfolioAllocations/treasury/REVERSEREPO.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### Gold Rate\n",
    "#### https://www.lbma.org.uk/prices-and-data/precious-metal-prices#/table\n",
    "gold_df = pd.read_csv('./inputFeatures/portfolioAllocations/commodities/gold.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "### JPY Rate\n",
    "#### Board of Governors of the Federal Reserve System (US),\n",
    "#### Japanese Yen to U.S. Dollar Spot Exchange Rate [DEXJPUS],\n",
    "#### retrieved from FRED, Federal Reserve Bank of St. Louis;\n",
    "#### https://fred.stlouisfed.org/series/DEXJPUS, April 3, 2022.\n",
    "jpy_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/JPY.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "         \n",
    "eur_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/EUR.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])\n",
    "\n",
    "gbp_df = pd.read_csv('./inputFeatures/portfolioAllocations/currency/GBP.csv',\n",
    "    index_col=['Date'],\n",
    "    parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Necessary functions\n",
    "def get_most_recent_value(date, lookup_df, column_name):\n",
    "    most_recent_date = [index for index in lookup_df.index if index <= date][-1]\n",
    "    return lookup_df[column_name].loc[most_recent_date]\n",
    "\n",
    "def calculate_percentage_change(index, number_of_days, lookup_df, column_name):\n",
    "    if (index + number_of_days < 0):\n",
    "        return None\n",
    "    if (index == combined_df.shape[0] - 1):\n",
    "        return None\n",
    "    final_value = lookup_df.iloc[max(index, index + number_of_days)][column_name]\n",
    "    starting_value = lookup_df.iloc[min(index, index + number_of_days)][column_name]\n",
    "    return (final_value - starting_value) / abs(starting_value) * 100\n",
    "\n",
    "dfs_to_combine = [\n",
    "    vix_df,\n",
    "    m1_df,\n",
    "    employment_df,\n",
    "    inflation_df,\n",
    "    gdp_df,\n",
    "    pcr_df,\n",
    "    umcsent_df,\n",
    "    confidence_df,\n",
    "    treasury_df,\n",
    "    effr_df,\n",
    "    repo_df,\n",
    "    reverse_repo_df,\n",
    "    gold_df,\n",
    "    jpy_df,\n",
    "    eur_df,\n",
    "    gbp_df]\n",
    "\n",
    "combined_df = spy_df\n",
    "combined_df = combined_df.join([df for df in dfs_to_combine])\n",
    "\n",
    "combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n",
    "\n",
    "for date in combined_df.index:\n",
    "    integer_location = combined_df.index.get_loc(date)\n",
    "\n",
    "    ### Fill in any gaps that may remain as a result of mismatched reported dates\n",
    "    ### This will repeat values in dates where there are extended periods without values\n",
    "    ### e.g. quarterly, monthly, weekly values\n",
    "    combined_df.at[date, 'M1Supply'] = get_most_recent_value(date, m1_df, 'M1Supply')\n",
    "    combined_df.at[date, 'EmploymentRate'] = get_most_recent_value(date, employment_df, 'EmploymentRate')\n",
    "    combined_df.at[date, 'InflationRate'] = get_most_recent_value(date, inflation_df, 'InflationRate')\n",
    "    combined_df.at[date, 'GDP'] = get_most_recent_value(date, gdp_df, 'GDP')\n",
    "    combined_df.at[date, 'UMCSENT'] = get_most_recent_value(date, umcsent_df, 'UMCSENT')\n",
    "    combined_df.at[date, 'Confidence'] = get_most_recent_value(date, confidence_df, 'Confidence')\n",
    "\n",
    "    ### Fill in gaps in treasury returns (due to bank holidays but market open)\n",
    "    ### by averaging the surrounding values if available\n",
    "    single_missing_value_columns = [\n",
    "        '1Mo', '3Mo', '1Yr', '2Yr', '5Yr', '10Yr', '20Yr', '30Yr',\n",
    "        'Repo', 'RepoRate', 'ReverseRepo', 'ReverseRepoRate', 'Price'\n",
    "    ]\n",
    "\n",
    "\n",
    "    for single_missing_value_column in single_missing_value_columns:\n",
    "        if (np.isnan(combined_df.at[date, single_missing_value_column])):\n",
    "            surrounding_values = combined_df.loc[\n",
    "                [combined_df.index[integer_location - 1], combined_df.index[integer_location + 1]],\n",
    "                single_missing_value_column\n",
    "            ].values\n",
    "            \n",
    "            if not (np.isnan(surrounding_values).any()):\n",
    "                combined_df.at[date, single_missing_value_column] = np.mean(surrounding_values)\n",
    "\n",
    "    combined_df['Repo'] = combined_df['Repo'].fillna(0.001)\n",
    "    combined_df['RepoRate'] = combined_df['RepoRate'].fillna(0.001)\n",
    "    combined_df['ReverseRepo'] = combined_df['ReverseRepo'].fillna(0.001)\n",
    "    combined_df['ReverseRepoRate'] = combined_df['ReverseRepoRate'].fillna(0.001)\n",
    "\n",
    "combined_df[combined_df < 0.001] = 0.001\n",
    "# combined_df.to_excel(\"./inputFeatures/combined.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 3324 validation: 144\n",
      "Dont buys: 1662, buys: 1662\n",
      "VALIDATION Dont buys: 72, buys: 72\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "from time import time\n",
    "\n",
    "SEQ_LEN = 5\n",
    "FUTURE_PERIOD_PREDICT = 1\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time())}\"\n",
    "DAYS_PERCENTAGE_CHANGE = [1, 2, 5, 21]\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def preprocess_df(df):\n",
    "    # 'RepoRate', 'ReverseRepoRate', 'Repo', 'ReverseRepo', 'USDGBP', 'USDEUR', 'USDJPY', 'M1Supply', 'EmploymentRate', 'InflationRate', 'GDP', 'PCR', 'UMCSENT', 'Confidence', 'EFFR'\n",
    "    df = df.drop(columns=['future'])\n",
    "\n",
    "    output_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col != 'target':\n",
    "            for number_of_days in DAYS_PERCENTAGE_CHANGE:\n",
    "                column_name = f'{col}{number_of_days}DaysChange'\n",
    "                output_df = pd.concat([output_df, df[col].pct_change(number_of_days, fill_method='ffill').rename(column_name)], axis=1)\n",
    "                output_df.dropna(inplace=True)\n",
    "                output_df[column_name] = preprocessing.scale(output_df[column_name].values)\n",
    "        elif col == 'target':\n",
    "            output_df[col] = df[col]\n",
    "\n",
    "    output_df.dropna(inplace=True)\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    \n",
    "    for i in output_df.values:  # iterate over the values\n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 21 sequences!\n",
    "            \n",
    "\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # append those bad boys!\n",
    "            \n",
    "    np.random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    ups = []\n",
    "    downs = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        \n",
    "        if target == 0:\n",
    "            downs.append([seq, target])\n",
    "        elif target == 1:\n",
    "            ups.append([seq, target])\n",
    "    np.random.shuffle(ups)\n",
    "    np.random.shuffle(downs)\n",
    "\n",
    "    lower = min(len(ups), len(downs))\n",
    "\n",
    "    ups = ups[:lower]\n",
    "    downs = downs[:lower]\n",
    "\n",
    "    sequential_data = ups + downs\n",
    "\n",
    "    np.random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "combined_df['future'] = combined_df['SPYClose'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "combined_df['target'] = list(map(classify, combined_df['SPYClose'], combined_df['future']))\n",
    "# print(combined_df[['future','SPYClose', 'target']])\n",
    "\n",
    "times = sorted(combined_df.index.values)\n",
    "\n",
    "last_5pct = sorted(combined_df.index.values)[-int(0.05*len(times) - max(DAYS_PERCENTAGE_CHANGE))]  # get the last 5% of the times\n",
    "\n",
    "## Split in sample / out of sample\n",
    "validation_df = combined_df[(combined_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
    "training_df = combined_df[(combined_df.index < last_5pct)]  # now the combined_df is all the data up to the last 5%\n",
    "\n",
    "train_x, train_y = preprocess_df(training_df)\n",
    "validation_x, validation_y = preprocess_df(validation_df)\n",
    "\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {np.count_nonzero(train_y == 0)}, buys: {np.count_nonzero(train_y == 1)}\")\n",
    "print(f\"VALIDATION Dont buys: {np.count_nonzero(validation_y == 0)}, buys: {np.count_nonzero(validation_y == 1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.8881 - accuracy: 0.4874\n",
      "Epoch 1: val_accuracy improved from -inf to 0.53472, saving model to models\\RNN_Final-01-0.535.hd5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_27_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\RNN_Final-01-0.535.hd5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\RNN_Final-01-0.535.hd5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89B818400> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89E72F7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89D13DF60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 12s 166ms/step - loss: 0.8873 - accuracy: 0.4874 - val_loss: 0.6918 - val_accuracy: 0.5347\n",
      "Epoch 2/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.7665 - accuracy: 0.5191\n",
      "Epoch 2: val_accuracy improved from 0.53472 to 0.54861, saving model to models\\RNN_Final-02-0.549.hd5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_27_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\RNN_Final-02-0.549.hd5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\RNN_Final-02-0.549.hd5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89B818400> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89E72F7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89D13DF60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 7s 142ms/step - loss: 0.7663 - accuracy: 0.5168 - val_loss: 0.6909 - val_accuracy: 0.5486\n",
      "Epoch 3/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.7217 - accuracy: 0.5555\n",
      "Epoch 3: val_accuracy did not improve from 0.54861\n",
      "52/52 [==============================] - 1s 14ms/step - loss: 0.7226 - accuracy: 0.5535 - val_loss: 0.6932 - val_accuracy: 0.4931\n",
      "Epoch 4/30\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 0.7147 - accuracy: 0.5456\n",
      "Epoch 4: val_accuracy did not improve from 0.54861\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.7143 - accuracy: 0.5448 - val_loss: 0.6886 - val_accuracy: 0.5069\n",
      "Epoch 5/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.6815 - accuracy: 0.5714\n",
      "Epoch 5: val_accuracy improved from 0.54861 to 0.56944, saving model to models\\RNN_Final-05-0.569.hd5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_26_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_27_layer_call_fn, lstm_cell_27_layer_call_and_return_conditional_losses, lstm_cell_28_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\RNN_Final-05-0.569.hd5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\RNN_Final-05-0.569.hd5\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89B818400> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89E72F7C0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000002B89D13DF60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 7s 137ms/step - loss: 0.6840 - accuracy: 0.5686 - val_loss: 0.6869 - val_accuracy: 0.5694\n",
      "Epoch 6/30\n",
      "48/52 [==========================>...] - ETA: 0s - loss: 0.6753 - accuracy: 0.5758\n",
      "Epoch 6: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 12ms/step - loss: 0.6733 - accuracy: 0.5809 - val_loss: 0.7150 - val_accuracy: 0.5000\n",
      "Epoch 7/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.6659 - accuracy: 0.5901\n",
      "Epoch 7: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.6657 - accuracy: 0.5924 - val_loss: 0.7204 - val_accuracy: 0.5486\n",
      "Epoch 8/30\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 0.6487 - accuracy: 0.6162\n",
      "Epoch 8: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.6510 - accuracy: 0.6128 - val_loss: 0.7314 - val_accuracy: 0.5069\n",
      "Epoch 9/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.6475 - accuracy: 0.6216\n",
      "Epoch 9: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.6479 - accuracy: 0.6215 - val_loss: 0.7319 - val_accuracy: 0.5139\n",
      "Epoch 10/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.6330 - accuracy: 0.6409\n",
      "Epoch 10: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.6340 - accuracy: 0.6405 - val_loss: 0.7616 - val_accuracy: 0.5000\n",
      "Epoch 11/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.6209 - accuracy: 0.6523\n",
      "Epoch 11: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.6209 - accuracy: 0.6528 - val_loss: 0.7449 - val_accuracy: 0.5417\n",
      "Epoch 12/30\n",
      "48/52 [==========================>...] - ETA: 0s - loss: 0.6099 - accuracy: 0.6696\n",
      "Epoch 12: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.6113 - accuracy: 0.6673 - val_loss: 0.8004 - val_accuracy: 0.4722\n",
      "Epoch 13/30\n",
      "48/52 [==========================>...] - ETA: 0s - loss: 0.5977 - accuracy: 0.6768\n",
      "Epoch 13: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5995 - accuracy: 0.6769 - val_loss: 0.7901 - val_accuracy: 0.4861\n",
      "Epoch 14/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.5952 - accuracy: 0.6776\n",
      "Epoch 14: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.5958 - accuracy: 0.6772 - val_loss: 0.7437 - val_accuracy: 0.5208\n",
      "Epoch 15/30\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 0.5692 - accuracy: 0.7025\n",
      "Epoch 15: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5711 - accuracy: 0.7001 - val_loss: 0.8347 - val_accuracy: 0.5278\n",
      "Epoch 16/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.5553 - accuracy: 0.7041\n",
      "Epoch 16: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5553 - accuracy: 0.7046 - val_loss: 0.7745 - val_accuracy: 0.5694\n",
      "Epoch 17/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.5347 - accuracy: 0.7331\n",
      "Epoch 17: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5381 - accuracy: 0.7289 - val_loss: 0.8631 - val_accuracy: 0.5278\n",
      "Epoch 18/30\n",
      "47/52 [==========================>...] - ETA: 0s - loss: 0.5367 - accuracy: 0.7158\n",
      "Epoch 18: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.5377 - accuracy: 0.7145 - val_loss: 0.8785 - val_accuracy: 0.5139\n",
      "Epoch 19/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.5120 - accuracy: 0.7459\n",
      "Epoch 19: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.5133 - accuracy: 0.7446 - val_loss: 0.8917 - val_accuracy: 0.4861\n",
      "Epoch 20/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.4942 - accuracy: 0.7503\n",
      "Epoch 20: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4954 - accuracy: 0.7482 - val_loss: 0.9556 - val_accuracy: 0.5278\n",
      "Epoch 21/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.4832 - accuracy: 0.7624\n",
      "Epoch 21: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4840 - accuracy: 0.7632 - val_loss: 0.9767 - val_accuracy: 0.5069\n",
      "Epoch 22/30\n",
      "48/52 [==========================>...] - ETA: 0s - loss: 0.4542 - accuracy: 0.7741\n",
      "Epoch 22: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4609 - accuracy: 0.7687 - val_loss: 1.0073 - val_accuracy: 0.4792\n",
      "Epoch 23/30\n",
      "47/52 [==========================>...] - ETA: 0s - loss: 0.4500 - accuracy: 0.7806\n",
      "Epoch 23: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4551 - accuracy: 0.7768 - val_loss: 1.0457 - val_accuracy: 0.5139\n",
      "Epoch 24/30\n",
      "50/52 [===========================>..] - ETA: 0s - loss: 0.4170 - accuracy: 0.7987\n",
      "Epoch 24: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.4175 - accuracy: 0.7981 - val_loss: 1.1116 - val_accuracy: 0.5208\n",
      "Epoch 25/30\n",
      "48/52 [==========================>...] - ETA: 0s - loss: 0.4073 - accuracy: 0.8092\n",
      "Epoch 25: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.4106 - accuracy: 0.8069 - val_loss: 1.2480 - val_accuracy: 0.4583\n",
      "Epoch 26/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.3836 - accuracy: 0.8205\n",
      "Epoch 26: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.3874 - accuracy: 0.8201 - val_loss: 1.4026 - val_accuracy: 0.4931\n",
      "Epoch 27/30\n",
      "49/52 [===========================>..] - ETA: 0s - loss: 0.3789 - accuracy: 0.8233\n",
      "Epoch 27: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.3811 - accuracy: 0.8222 - val_loss: 1.4504 - val_accuracy: 0.5208\n",
      "Epoch 28/30\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.3627 - accuracy: 0.8346\n",
      "Epoch 28: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.3626 - accuracy: 0.8348 - val_loss: 1.5046 - val_accuracy: 0.4792\n",
      "Epoch 29/30\n",
      "47/52 [==========================>...] - ETA: 0s - loss: 0.3354 - accuracy: 0.8467\n",
      "Epoch 29: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.3406 - accuracy: 0.8433 - val_loss: 1.4911 - val_accuracy: 0.4931\n",
      "Epoch 30/30\n",
      "48/52 [==========================>...] - ETA: 0s - loss: 0.3295 - accuracy: 0.8594\n",
      "Epoch 30: val_accuracy did not improve from 0.56944\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.3328 - accuracy: 0.8574 - val_loss: 1.5000 - val_accuracy: 0.5069\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dropout, BatchNormalization, Dense,MaxPooling1D\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time())}\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Conv1D(64,2,activation='relu', padding='same', input_shape=(train_x.shape[1:])))\n",
    "# model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f'newlogs/{NAME}')\n",
    "\n",
    "checkpoint_filepath = \"models/RNN_Final-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.28239939 -0.13149457  0.75962336 ...  0.23903523  0.24355261\n",
      "    0.22120402]\n",
      "  [ 1.43573839  0.08758815  0.87234258 ...  0.66146084 -0.02068673\n",
      "    0.44132751]\n",
      "  [ 0.44478501  1.27394354  0.74221211 ...  1.01584242  0.54390839\n",
      "   -0.59659049]\n",
      "  [ 0.66157668  0.74753166  1.07188525 ... -0.91866029 -0.09459053\n",
      "   -0.15773054]\n",
      "  [ 1.17284571  1.24369429  1.10155193 ... -2.09289624 -0.68912757\n",
      "   -0.27598266]]\n",
      "\n",
      " [[-0.05370345  0.14228749  0.21368425 ... -0.1676948   0.37005854\n",
      "   -0.39130015]\n",
      "  [-0.37778233 -0.29531534 -0.13540727 ... -0.28899721  0.03160059\n",
      "   -0.20531082]\n",
      "  [-0.14855984 -0.35927251 -0.12390169 ... -0.75180087 -0.66232194\n",
      "    0.63126068]\n",
      "  [-0.4463075  -0.40558508 -0.3337655  ... -0.8143052  -0.73112248\n",
      "    0.8114754 ]\n",
      "  [-0.79679476 -0.84235006 -0.80833548 ...  0.01410849 -0.6277626\n",
      "    0.8092381 ]]\n",
      "\n",
      " [[ 1.53808426  0.79842481 -0.5300499  ...  1.28513659  0.90855506\n",
      "    0.17072279]\n",
      "  [ 0.0788917   1.09195372 -1.10592052 ... -0.14475761  0.66279771\n",
      "    0.36440077]\n",
      "  [-2.10728882 -1.37753314 -1.25342242 ...  1.38499071  1.52481672\n",
      "    0.80978429]\n",
      "  [-0.65973409 -1.86666398 -0.67912158 ...  1.43678817  1.73039041\n",
      "    0.46942782]\n",
      "  [-1.07364682 -1.17175621 -1.00167887 ...  0.30735688  1.04430446\n",
      "    0.65383456]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.0788917   1.09195372 -1.10592052 ... -0.14475761  0.66279771\n",
      "    0.36440077]\n",
      "  [-2.10728882 -1.37753314 -1.25342242 ...  1.38499071  1.52481672\n",
      "    0.80978429]\n",
      "  [-0.65973409 -1.86666398 -0.67912158 ...  1.43678817  1.73039041\n",
      "    0.46942782]\n",
      "  [-1.07364682 -1.17175621 -1.00167887 ...  0.30735688  1.04430446\n",
      "    0.65383456]\n",
      "  [-1.75501063 -1.90494555 -2.42681421 ...  0.32100957  1.08749563\n",
      "    0.59406421]]\n",
      "\n",
      " [[ 1.43328369 -0.23866705 -1.85172291 ...  3.26428698  3.3280676\n",
      "    1.90767816]\n",
      "  [ 2.1154133   2.41975442  0.00799722 ...  0.67005784  0.61654093\n",
      "    0.75509827]\n",
      "  [-0.27888967  1.23561165  0.18054288 ... -2.93457184  0.33875668\n",
      "    1.14171225]\n",
      "  [-1.5110945  -1.21206632 -0.01928491 ...  0.2953996   0.60863127\n",
      "    1.92261918]\n",
      "  [ 1.75875625  0.14575195  1.5899163  ...  0.95074641  0.75324216\n",
      "    2.01416318]]\n",
      "\n",
      " [[ 0.0215825  -0.1066993   0.2203449  ...  0.35118641  0.45391419\n",
      "   -0.3320333 ]\n",
      "  [ 0.26907325  0.19338813  0.61764704 ... -0.03911309  0.84336257\n",
      "   -0.35863758]\n",
      "  [-0.05370345  0.14228749  0.21368425 ... -0.1676948   0.37005854\n",
      "   -0.39130015]\n",
      "  [-0.37778233 -0.29531534 -0.13540727 ... -0.28899721  0.03160059\n",
      "   -0.20531082]\n",
      "  [-0.14855984 -0.35927251 -0.12390169 ... -0.75180087 -0.66232194\n",
      "    0.63126068]]]\n"
     ]
    }
   ],
   "source": [
    "print(validation_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise All Inputs between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalise(amount, column_name):\n",
    "#     range = combined_df[column_name].max() - combined_df[column_name].min()\n",
    "#     return (amount - combined_df[column_name].min()) / range \n",
    "\n",
    "# def posneg(amount):\n",
    "#     if amount >= 0:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# normalised_df = pd.DataFrame()\n",
    "# normalised_df['Volume'] = combined_df['Volume'].apply(normalise, args=('Volume',))\n",
    "# normalised_df['VIXClose'] = combined_df['VIXClose'].apply(normalise, args=('VIXClose',))\n",
    "# normalised_df['M1Supply'] = combined_df['M1Supply'].apply(normalise, args=('M1Supply',))\n",
    "# normalised_df['EmploymentRate'] = combined_df['EmploymentRate'].apply(normalise, args=('EmploymentRate',))\n",
    "# normalised_df['InflationRate'] = combined_df['InflationRate'].apply(normalise, args=('InflationRate',))\n",
    "# normalised_df['GDP'] = combined_df['GDP'].apply(normalise, args=('GDP',))\n",
    "# normalised_df['PCR'] = combined_df['PCR'].apply(normalise, args=('PCR',))\n",
    "# normalised_df['UMCSENT'] = combined_df['UMCSENT'].apply(normalise, args=('UMCSENT',))\n",
    "# normalised_df['Confidence'] = combined_df['Confidence'].apply(normalise, args=('Confidence',))\n",
    "# normalised_df['1Mo'] = combined_df['1Mo'].apply(normalise, args=('1Mo',))\n",
    "# normalised_df['3Mo'] = combined_df['3Mo'].apply(normalise, args=('3Mo',))\n",
    "# normalised_df['1Yr'] = combined_df['1Yr'].apply(normalise, args=('1Yr',))\n",
    "# normalised_df['2Yr'] = combined_df['2Yr'].apply(normalise, args=('2Yr',))\n",
    "# normalised_df['5Yr'] = combined_df['5Yr'].apply(normalise, args=('5Yr',))\n",
    "# normalised_df['10Yr'] = combined_df['10Yr'].apply(normalise, args=('10Yr',))\n",
    "# normalised_df['20Yr'] = combined_df['20Yr'].apply(normalise, args=('20Yr',))\n",
    "# normalised_df['30Yr'] = combined_df['30Yr'].apply(normalise, args=('30Yr',))\n",
    "# normalised_df['EFFR'] = combined_df['EFFR'].apply(normalise, args=('EFFR',))\n",
    "# normalised_df['Repo'] = combined_df['Repo'].apply(normalise, args=('Repo',))\n",
    "# normalised_df['RepoRate'] = combined_df['RepoRate'].apply(normalise, args=('RepoRate',))\n",
    "# normalised_df['ReverseRepo'] = combined_df['ReverseRepo'].apply(normalise, args=('ReverseRepo',))\n",
    "# normalised_df['ReverseRepoRate'] = combined_df['ReverseRepoRate'].apply(normalise, args=('ReverseRepoRate',))\n",
    "# normalised_df['Price'] = combined_df['Price'].apply(normalise, args=('Price',))\n",
    "# normalised_df['USDJPY'] = combined_df['USDJPY'].apply(normalise, args=('USDJPY',))\n",
    "# normalised_df['USDEUR'] = combined_df['USDEUR'].apply(normalise, args=('USDEUR',))\n",
    "# normalised_df['USDGBP'] = combined_df['USDGBP'].apply(normalise, args=('USDGBP',))\n",
    "# normalised_df['SPY1DayChange'] = combined_df['SPY1DayChange'].apply(normalise, args=('SPY1DayChange',))\n",
    "# normalised_df['SPY2DayChange'] = combined_df['SPY2DayChange'].apply(normalise, args=('SPY2DayChange',))\n",
    "# normalised_df['SPY21DayChange'] = combined_df['SPY21DayChange'].apply(normalise, args=('SPY21DayChange',))\n",
    "# normalised_df['SPY63DayChange'] = combined_df['SPY63DayChange'].apply(normalise, args=('SPY63DayChange',))\n",
    "# normalised_df['SPY126DayChange'] = combined_df['SPY126DayChange'].apply(normalise, args=('SPY126DayChange',))\n",
    "# normalised_df['SPY252DayChange'] = combined_df['SPY252DayChange'].apply(normalise, args=('SPY252DayChange',))\n",
    "# normalised_df['SPYNextDayChange'] = combined_df['SPYNextDayChange'].apply(normalise, args=('SPYNextDayChange',))\n",
    "# normalised_df['SPYNextDayDirection'] = combined_df['SPYNextDayChange'].apply(posneg)\n",
    "\n",
    "# normalised_df.to_excel(\"./inputFeatures/normalised.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_of_days = 21\n",
    "\n",
    "# def group_days(lookup_df, start_index, end_index, input_start_column, input_end_column, output_column, number_of_days):\n",
    "#     X = []\n",
    "#     y = []\n",
    "\n",
    "#     full_inputs = lookup_df.iloc[start_index:end_index, input_start_column:input_end_column].to_numpy()\n",
    "#     full_outputs = lookup_df.iloc[start_index + number_of_days - 1 : end_index + number_of_days, output_column].to_numpy()\n",
    "\n",
    "#     for index in range(len(full_inputs) - number_of_days + 1):\n",
    "#         X_rows = []\n",
    "#         for day in range(number_of_days):\n",
    "#             if index + day > len(full_inputs) - 1:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 X_rows.append(full_inputs[index+day])\n",
    "#                 if day == 2:\n",
    "#                     y.append(full_outputs[index])\n",
    "#         if np.any(X_rows):\n",
    "#             X.append(X_rows)\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # print(np.asarray(normalised_df.iloc[:, 0:32].to_numpy))\n",
    "\n",
    "# X, y = group_days(normalised_df, 300, 600, 0, 32, 33, no_of_days)\n",
    "\n",
    "# X = X.reshape((X.shape[0],1, no_of_days,32, 1))\n",
    "\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = X.reshape((X.shape[0],1, no_of_days,32, 1))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.45, random_state = 42)\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "# import time\n",
    "\n",
    "# NAME = \"stonky-{}\".format(int(time.time()))\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import *\n",
    "# # define model\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(1,no_of_days,32,1)))\n",
    "# model.add(TimeDistributed((Conv2D(64, 3, activation='relu'))))\n",
    "# model.add(TimeDistributed(MaxPooling2D(2, 2)))\n",
    "\n",
    "# model.add(TimeDistributed(MaxPooling2D(2, 2)))\n",
    "# model.add(TimeDistributed(Conv2D(64, 3, activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling2D(2, 2)))\n",
    "# model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# # LSTM layers\n",
    "\n",
    "# model.add((LSTM(no_of_days, return_sequences=True)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add((LSTM(no_of_days, return_sequences=False)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # #Final layers\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'mse', 'mae'])\n",
    "\n",
    "# model.build()\n",
    "# model.summary()\n",
    "\n",
    "# history = model.fit(X_train, y_train, validation_split=0.3, epochs=100, batch_size=32, callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = (combined_df.iloc[500:752])\n",
    "\n",
    "# normalised_df = pd.DataFrame()\n",
    "# normalised_df['Volume'] = temp_df['Volume'].apply(normalise, args=('Volume',))\n",
    "# normalised_df['VIXClose'] = temp_df['VIXClose'].apply(normalise, args=('VIXClose',))\n",
    "# normalised_df['M1Supply'] = temp_df['M1Supply'].apply(normalise, args=('M1Supply',))\n",
    "# normalised_df['EmploymentRate'] = temp_df['EmploymentRate'].apply(normalise, args=('EmploymentRate',))\n",
    "# normalised_df['InflationRate'] = temp_df['InflationRate'].apply(normalise, args=('InflationRate',))\n",
    "# normalised_df['GDP'] = temp_df['GDP'].apply(normalise, args=('GDP',))\n",
    "# normalised_df['PCR'] = temp_df['PCR'].apply(normalise, args=('PCR',))\n",
    "# normalised_df['UMCSENT'] = temp_df['UMCSENT'].apply(normalise, args=('UMCSENT',))\n",
    "# normalised_df['Confidence'] = temp_df['Confidence'].apply(normalise, args=('Confidence',))\n",
    "# normalised_df['1Mo'] = temp_df['1Mo'].apply(normalise, args=('1Mo',))\n",
    "# normalised_df['3Mo'] = temp_df['3Mo'].apply(normalise, args=('3Mo',))\n",
    "# normalised_df['1Yr'] = temp_df['1Yr'].apply(normalise, args=('1Yr',))\n",
    "# normalised_df['2Yr'] = temp_df['2Yr'].apply(normalise, args=('2Yr',))\n",
    "# normalised_df['5Yr'] = temp_df['5Yr'].apply(normalise, args=('5Yr',))\n",
    "# normalised_df['10Yr'] = temp_df['10Yr'].apply(normalise, args=('10Yr',))\n",
    "# normalised_df['20Yr'] = temp_df['20Yr'].apply(normalise, args=('20Yr',))\n",
    "# normalised_df['30Yr'] = temp_df['30Yr'].apply(normalise, args=('30Yr',))\n",
    "# normalised_df['EFFR'] = temp_df['EFFR'].apply(normalise, args=('EFFR',))\n",
    "# normalised_df['Repo'] = temp_df['Repo'].apply(normalise, args=('Repo',))\n",
    "# normalised_df['RepoRate'] = temp_df['RepoRate'].apply(normalise, args=('RepoRate',))\n",
    "# normalised_df['ReverseRepo'] = temp_df['ReverseRepo'].apply(normalise, args=('ReverseRepo',))\n",
    "# normalised_df['ReverseRepoRate'] = temp_df['ReverseRepoRate'].apply(normalise, args=('ReverseRepoRate',))\n",
    "# normalised_df['Price'] = temp_df['Price'].apply(normalise, args=('Price',))\n",
    "# normalised_df['USDJPY'] = temp_df['USDJPY'].apply(normalise, args=('USDJPY',))\n",
    "# normalised_df['USDEUR'] = temp_df['USDEUR'].apply(normalise, args=('USDEUR',))\n",
    "# normalised_df['USDGBP'] = temp_df['USDGBP'].apply(normalise, args=('USDGBP',))\n",
    "# normalised_df['SPY1DayChange'] = temp_df['SPY1DayChange'].apply(normalise, args=('SPY1DayChange',))\n",
    "# normalised_df['SPY2DayChange'] = temp_df['SPY2DayChange'].apply(normalise, args=('SPY2DayChange',))\n",
    "# normalised_df['SPY21DayChange'] = temp_df['SPY21DayChange'].apply(normalise, args=('SPY21DayChange',))\n",
    "# normalised_df['SPY63DayChange'] = temp_df['SPY63DayChange'].apply(normalise, args=('SPY63DayChange',))\n",
    "# normalised_df['SPY126DayChange'] = temp_df['SPY126DayChange'].apply(normalise, args=('SPY126DayChange',))\n",
    "# normalised_df['SPY252DayChange'] = temp_df['SPY252DayChange'].apply(normalise, args=('SPY252DayChange',))\n",
    "# normalised_df['SPYNextDayChange'] = temp_df['SPYNextDayChange'].apply(normalise, args=('SPYNextDayChange',))\n",
    "# normalised_df['SPYNextDayDirection'] = temp_df['SPYNextDayChange'].apply(posneg)\n",
    "\n",
    "# model = Sequential()\n",
    "# from keras.layers import *\n",
    "\n",
    "# training_data = []\n",
    "\n",
    "# def add_training_row(row):\n",
    "#     return (row[0:32].to_numpy(), row[33])\n",
    "\n",
    "# for index, row in normalised_df.iterrows():\n",
    "#     training_data.append(add_training_row(row))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
